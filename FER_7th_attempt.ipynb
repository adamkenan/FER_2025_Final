{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "835a7772",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "FER with Vision Transformer Backbone\n",
    "Modified from 5th attempt - ONLY model architecture changed\n",
    "Everything else (augmentation, training, loss, etc.) remains identical\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from torchvision import transforms, models\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  1. Label Smoothing Loss (UNCHANGED)\n",
    "# ============================================================\n",
    "\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    \"\"\"Label smoothing prevents overconfident predictions.\"\"\"\n",
    "    def __init__(self, epsilon: float = 0.1, weight=None):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.weight = weight\n",
    "        \n",
    "    def forward(self, outputs, targets):\n",
    "        n_classes = outputs.size(-1)\n",
    "        log_preds = F.log_softmax(outputs, dim=-1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(log_preds)\n",
    "            true_dist.fill_(self.epsilon / (n_classes - 1))\n",
    "            true_dist.scatter_(1, targets.unsqueeze(1), 1.0 - self.epsilon)\n",
    "            \n",
    "            if self.weight is not None:\n",
    "                true_dist = true_dist * self.weight.unsqueeze(0)\n",
    "        \n",
    "        loss = torch.sum(-true_dist * log_preds, dim=-1)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  2. Mixup & CutMix (UNCHANGED)\n",
    "# ============================================================\n",
    "\n",
    "def mixup_data(x, y, alpha=0.4, device='cuda'):\n",
    "    \"\"\"Enhanced mixup for better generalization\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "        lam = max(lam, 1 - lam)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "def cutmix_data(x, y, alpha=1.0, device='cuda'):\n",
    "    \"\"\"CutMix augmentation for facial features.\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "\n",
    "    W, H = x.size(2), x.size(3)\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    x_cutmix = x.clone()\n",
    "    x_cutmix[:, :, bbx1:bbx2, bby1:bby2] = x[index, :, bbx1:bbx2, bby1:bby2]\n",
    "    \n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (W * H))\n",
    "    \n",
    "    y_a, y_b = y, y[index]\n",
    "    return x_cutmix, y_a, y_b, lam\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  3. Data Collection (UNCHANGED)\n",
    "# ============================================================\n",
    "\n",
    "def gather_image_paths_and_labels(root_dir: str) -> Tuple[List[str], List[int], List[str]]:\n",
    "    \"\"\"Collect all images from emotion class subdirectories.\"\"\"\n",
    "    root = Path(root_dir)\n",
    "    if not root.exists():\n",
    "        raise FileNotFoundError(f\"Dataset root not found: {root_dir}\")\n",
    "\n",
    "    image_data = []\n",
    "    exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\"}\n",
    "    \n",
    "    for img_path in root.rglob(\"*\"):\n",
    "        if img_path.suffix.lower() in exts:\n",
    "            emotion_class = img_path.parent.name\n",
    "            if emotion_class in ['train', 'test']:\n",
    "                emotion_class = img_path.parent.parent.name\n",
    "            \n",
    "            image_data.append((str(img_path), emotion_class))\n",
    "    \n",
    "    if not image_data:\n",
    "        raise ValueError(f\"No images found under {root_dir}\")\n",
    "    \n",
    "    unique_classes = sorted(set(emotion for _, emotion in image_data))\n",
    "    class_to_idx = {name: idx for idx, name in enumerate(unique_classes)}\n",
    "    \n",
    "    image_paths = [path for path, _ in image_data]\n",
    "    labels = [class_to_idx[emotion] for _, emotion in image_data]\n",
    "    \n",
    "    return image_paths, labels, unique_classes\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  4. Pre-cached Dataset (UNCHANGED)\n",
    "# ============================================================\n",
    "\n",
    "class PreCachedImageDataset(Dataset):\n",
    "    \"\"\"Ultra-fast dataset with pre-cached tensors.\"\"\"\n",
    "    def __init__(self, image_paths: List[str], labels: List[int], \n",
    "                 transform=None, cache_images: bool = True, img_size: int = 224,\n",
    "                 is_train: bool = True):\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "        self.cached_tensors = []\n",
    "        \n",
    "        if cache_images:\n",
    "            print(f\"Pre-loading {len(image_paths)} images...\")\n",
    "            \n",
    "            imagenet_mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "            imagenet_std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "            \n",
    "            resize_transform = transforms.Compose([\n",
    "                transforms.Resize(img_size),\n",
    "                transforms.ToTensor(),\n",
    "            ])\n",
    "            \n",
    "            for path in tqdm(image_paths, desc=\"Caching\"):\n",
    "                try:\n",
    "                    img = Image.open(path).convert(\"RGB\")\n",
    "                    img_tensor = resize_transform(img)\n",
    "                    \n",
    "                    if not is_train:\n",
    "                        img_tensor = (img_tensor - imagenet_mean) / imagenet_std\n",
    "                    \n",
    "                    self.cached_tensors.append(img_tensor)\n",
    "                except Exception as e:\n",
    "                    blank = torch.zeros(3, img_size, img_size)\n",
    "                    if not is_train:\n",
    "                        blank = (blank - imagenet_mean) / imagenet_std\n",
    "                    self.cached_tensors.append(blank)\n",
    "        else:\n",
    "            self.cached_tensors = None\n",
    "            self.image_paths = image_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.cached_tensors is not None:\n",
    "            img = self.cached_tensors[idx]\n",
    "            if self.is_train and self.transform:\n",
    "                img = self.transform(img)\n",
    "        else:\n",
    "            img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "        \n",
    "        return img, self.labels[idx]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  5. Augmentation Transforms (UNCHANGED)\n",
    "# ============================================================\n",
    "\n",
    "def get_augmentation_transforms() -> Tuple[transforms.Compose, transforms.Compose]:\n",
    "    \"\"\"Strong augmentation strategy for better generalization.\"\"\"\n",
    "    imagenet_mean = [0.485, 0.456, 0.406]\n",
    "    imagenet_std = [0.229, 0.224, 0.225]\n",
    "    \n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.1),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.15, 0.15), scale=(0.85, 1.15)),\n",
    "        transforms.RandomPerspective(distortion_scale=0.2, p=0.3),\n",
    "        transforms.Normalize(imagenet_mean, imagenet_std),\n",
    "        transforms.RandomErasing(p=0.3, scale=(0.02, 0.2)),\n",
    "    ])\n",
    "    \n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Normalize(imagenet_mean, imagenet_std),\n",
    "    ])\n",
    "    \n",
    "    return train_transform, val_transform\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  6. MODIFIED: ViT Model with Same Classifier Head\n",
    "# ============================================================\n",
    "\n",
    "def build_model(model_name: str = 'vit_b_16', num_classes: int = 7, \n",
    "                dropout_rate: float = 0.5) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Build Vision Transformer model with custom classifier head.\n",
    "    \n",
    "    CHANGES FROM ORIGINAL:\n",
    "    - Replaced ResNet-34 backbone with Vision Transformer (ViT-B/16)\n",
    "    - Kept EXACT SAME 3-layer classifier head architecture\n",
    "    - All other parameters (dropout, batch norm, layer sizes) identical\n",
    "    \n",
    "    Args:\n",
    "        model_name: ViT variant ('vit_b_16', 'vit_b_32', 'vit_l_16')\n",
    "        num_classes: Number of emotion classes (7)\n",
    "        dropout_rate: Dropout probability (0.5, same as ResNet version)\n",
    "    \n",
    "    Returns:\n",
    "        ViT model with custom 3-layer MLP classifier\n",
    "    \"\"\"\n",
    "    # Load pre-trained Vision Transformer\n",
    "    if model_name == 'vit_b_16':\n",
    "        weights = models.ViT_B_16_Weights.IMAGENET1K_V1\n",
    "        model = models.vit_b_16(weights=weights)\n",
    "        hidden_dim = 768  # ViT-B/16 output dimension\n",
    "    elif model_name == 'vit_b_32':\n",
    "        weights = models.ViT_B_32_Weights.IMAGENET1K_V1\n",
    "        model = models.vit_b_32(weights=weights)\n",
    "        hidden_dim = 768  # ViT-B/32 output dimension\n",
    "    elif model_name == 'vit_l_16':\n",
    "        weights = models.ViT_L_16_Weights.IMAGENET1K_V1\n",
    "        model = models.vit_l_16(weights=weights)\n",
    "        hidden_dim = 1024  # ViT-L/16 output dimension\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "\n",
    "    # Replace classifier head with IDENTICAL architecture to ResNet version\n",
    "    # Original ResNet: 512 → 1024 → 512 → 7\n",
    "    # ViT version:     768 → 1024 → 512 → 7 (only input dim changes)\n",
    "    model.heads = nn.Sequential(\n",
    "        nn.BatchNorm1d(hidden_dim),              # BatchNorm on ViT features\n",
    "        nn.Dropout(dropout_rate * 0.5),          # 0.25 dropout (same as original)\n",
    "        nn.Linear(hidden_dim, 1024),             # 768→1024 (ResNet had 512→1024)\n",
    "        nn.BatchNorm1d(1024),                    # Same as original\n",
    "        nn.ReLU(inplace=True),                   # Same activation\n",
    "        nn.Dropout(dropout_rate),                # 0.5 dropout (same as original)\n",
    "        nn.Linear(1024, 512),                    # Same as original\n",
    "        nn.BatchNorm1d(512),                     # Same as original\n",
    "        nn.ReLU(inplace=True),                   # Same activation\n",
    "        nn.Dropout(dropout_rate),                # 0.5 dropout (same as original)\n",
    "        nn.Linear(512, num_classes)              # Same final layer\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  7. Training Function (UNCHANGED)\n",
    "# ============================================================\n",
    "\n",
    "def train_one_epoch(model: nn.Module, loader: DataLoader, criterion: nn.Module,\n",
    "                   optimizer: optim.Optimizer, device: torch.device, epoch: int,\n",
    "                   scaler: GradScaler = None, use_mixup: bool = True, \n",
    "                   mixup_alpha: float = 0.4, use_amp: bool = False,\n",
    "                   use_cutmix: bool = True) -> Tuple[float, float]:\n",
    "    \"\"\"Training with mixup/cutmix alternation.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch} [Train]\", leave=False)\n",
    "\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        use_aug = random.random() < 0.5\n",
    "        use_cutmix_now = use_cutmix and random.random() < 0.5\n",
    "        \n",
    "        if use_amp and scaler is not None:\n",
    "            with autocast():\n",
    "                if use_aug and use_mixup:\n",
    "                    if use_cutmix_now:\n",
    "                        mixed_images, labels_a, labels_b, lam = cutmix_data(images, labels, 1.0, device)\n",
    "                    else:\n",
    "                        mixed_images, labels_a, labels_b, lam = mixup_data(images, labels, mixup_alpha, device)\n",
    "                    \n",
    "                    outputs = model(mixed_images)\n",
    "                    loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)\n",
    "                    \n",
    "                    preds = outputs.argmax(dim=1)\n",
    "                    correct += (lam * (preds == labels_a).sum().item() + (1 - lam) * (preds == labels_b).sum().item())\n",
    "                else:\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    preds = outputs.argmax(dim=1)\n",
    "                    correct += (preds == labels).sum().item()\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            if use_aug and use_mixup:\n",
    "                if use_cutmix_now:\n",
    "                    mixed_images, labels_a, labels_b, lam = cutmix_data(images, labels, 1.0, device)\n",
    "                else:\n",
    "                    mixed_images, labels_a, labels_b, lam = mixup_data(images, labels, mixup_alpha, device)\n",
    "                \n",
    "                outputs = model(mixed_images)\n",
    "                loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)\n",
    "                \n",
    "                preds = outputs.argmax(dim=1)\n",
    "                correct += (lam * (preds == labels_a).sum().item() + (1 - lam) * (preds == labels_b).sum().item())\n",
    "            else:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                preds = outputs.argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        total += images.size(0)\n",
    "\n",
    "        if pbar.n % max(1, len(loader) // 20) == 0:\n",
    "            avg_loss = running_loss / (pbar.n + 1)\n",
    "            avg_acc = 100.0 * correct / total\n",
    "            pbar.set_postfix({\"loss\": f\"{avg_loss:.4f}\", \"acc\": f\"{avg_acc:.2f}%\"})\n",
    "\n",
    "    avg_loss = running_loss / len(loader)\n",
    "    avg_acc = 100.0 * correct / total\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  8. Evaluation Function (UNCHANGED)\n",
    "# ============================================================\n",
    "\n",
    "def evaluate(model: nn.Module, loader: DataLoader, criterion: nn.Module,\n",
    "            device: torch.device, epoch: int, use_amp: bool = False,\n",
    "            use_tta: bool = False) -> Tuple[float, float]:\n",
    "    \"\"\"Evaluation with optional TTA.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch} [Val]\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in pbar:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            if use_amp:\n",
    "                with autocast():\n",
    "                    if use_tta:\n",
    "                        outputs1 = model(images)\n",
    "                        outputs2 = model(torch.flip(images, dims=[3]))\n",
    "                        outputs = (outputs1 + outputs2) / 2\n",
    "                    else:\n",
    "                        outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "            else:\n",
    "                if use_tta:\n",
    "                    outputs1 = model(images)\n",
    "                    outputs2 = model(torch.flip(images, dims=[3]))\n",
    "                    outputs = (outputs1 + outputs2) / 2\n",
    "                else:\n",
    "                    outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += images.size(0)\n",
    "\n",
    "            if pbar.n % max(1, len(loader) // 10) == 0:\n",
    "                avg_loss = running_loss / (pbar.n + 1)\n",
    "                avg_acc = 100.0 * correct / total\n",
    "                pbar.set_postfix({\"loss\": f\"{avg_loss:.4f}\", \"acc\": f\"{avg_acc:.2f}%\"})\n",
    "\n",
    "    avg_loss = running_loss / len(loader)\n",
    "    avg_acc = 100.0 * correct / total\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  9. Training Loop with OneCycleLR (UNCHANGED)\n",
    "# ============================================================\n",
    "\n",
    "def train_advanced(model, train_loader, val_loader, criterion, \n",
    "                   device, CONFIG, class_names):\n",
    "    \"\"\"Advanced training with OneCycleLR.\"\"\"\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=CONFIG['lr'],\n",
    "        weight_decay=CONFIG['weight_decay']\n",
    "    )\n",
    "    \n",
    "    total_steps = len(train_loader) * CONFIG['epochs']\n",
    "    scheduler = OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=CONFIG['lr'] * 10,\n",
    "        total_steps=total_steps,\n",
    "        pct_start=0.3,\n",
    "        anneal_strategy='cos',\n",
    "        div_factor=25.0,\n",
    "        final_div_factor=10000.0\n",
    "    )\n",
    "    \n",
    "    use_amp = torch.cuda.is_available()\n",
    "    scaler = GradScaler() if use_amp else None\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Starting high-accuracy training with ViT backbone...\")\n",
    "    if use_amp:\n",
    "        print(\"Mixed Precision: ENABLED\")\n",
    "    else:\n",
    "        print(\"Mixed Precision: DISABLED (CPU mode)\")\n",
    "    print(f\"Target: 75-80% validation accuracy\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for epoch in range(1, CONFIG['epochs'] + 1):\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, device, epoch,\n",
    "            scaler, use_mixup=CONFIG['use_mixup'], \n",
    "            mixup_alpha=CONFIG['mixup_alpha'], use_amp=use_amp,\n",
    "            use_cutmix=CONFIG['use_cutmix']\n",
    "        )\n",
    "        \n",
    "        use_tta = CONFIG['use_tta'] and epoch > 20\n",
    "        val_loss, val_acc = evaluate(\n",
    "            model, val_loader, criterion, device, epoch, \n",
    "            use_amp=use_amp, use_tta=use_tta\n",
    "        )\n",
    "        \n",
    "        gap = train_acc - val_acc\n",
    "        \n",
    "        print(f\"\\n[Epoch {epoch}/{CONFIG['epochs']}]\")\n",
    "        print(f\"  Train -> Loss: {train_loss:.4f}  Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Val   -> Loss: {val_loss:.4f}  Acc: {val_acc:.2f}%\")\n",
    "        print(f\"  Gap: {gap:.2f}% | LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        if use_tta:\n",
    "            print(f\"  TTA: Enabled\")\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = epoch\n",
    "            patience_counter = 0\n",
    "            save_checkpoint(model, optimizer, epoch, val_acc, val_loss, \n",
    "                          class_names, CONFIG)\n",
    "            print(f\"  ✓ New best! Val Acc: {val_acc:.2f}% (Gap: {gap:.2f}%)\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  No improvement ({patience_counter}/{CONFIG['patience']})\")\n",
    "            \n",
    "            if patience_counter >= CONFIG['patience']:\n",
    "                print(f\"\\n{'='*60}\")\n",
    "                print(f\"Early stopping triggered\")\n",
    "                print(\"=\"*60)\n",
    "                break\n",
    "    \n",
    "    return best_val_acc, best_epoch\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, val_acc, val_loss, class_names, CONFIG):\n",
    "    \"\"\"Save model checkpoint\"\"\"\n",
    "    best_path = os.path.join(CONFIG['save_dir'], \"best_model_vit.pth\")\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'val_acc': val_acc,\n",
    "        'val_loss': val_loss,\n",
    "        'class_names': class_names,\n",
    "        'config': CONFIG\n",
    "    }, best_path)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  10. Main Pipeline\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    # ============= Configuration (ONLY model_name changed) =============\n",
    "    CONFIG = {\n",
    "        'data_root': \"C:/adam/AMIT_Diploma/grad_project/FER/archive_v1\",\n",
    "        'model_name': 'vit_b_16',  # CHANGED: ViT-B/16 instead of resnet34\n",
    "        'img_size': 224,\n",
    "        'batch_size': 64,\n",
    "        'epochs': 60,\n",
    "        'lr': 3e-4,\n",
    "        'weight_decay': 2e-4,\n",
    "        'dropout': 0.5,\n",
    "        'val_size': 0.15,\n",
    "        'random_seed': 42,\n",
    "        'num_workers': 0,\n",
    "        'patience': 12,\n",
    "        'use_class_weights': True,\n",
    "        'use_label_smoothing': True,\n",
    "        'label_smooth_eps': 0.1,\n",
    "        'use_mixup': True,\n",
    "        'mixup_alpha': 0.4,\n",
    "        'use_cutmix': True,\n",
    "        'use_tta': True,\n",
    "        'save_dir': './checkpoints',\n",
    "        'cache_images': True,\n",
    "    }\n",
    "\n",
    "    # Setup\n",
    "    random.seed(CONFIG['random_seed'])\n",
    "    np.random.seed(CONFIG['random_seed'])\n",
    "    torch.manual_seed(CONFIG['random_seed'])\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(CONFIG['random_seed'])\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    os.makedirs(CONFIG['save_dir'], exist_ok=True)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "    # Load Data\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Loading dataset...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    image_paths, labels, class_names = gather_image_paths_and_labels(CONFIG['data_root'])\n",
    "    num_classes = len(class_names)\n",
    "    \n",
    "    print(f\"\\nDataset Statistics:\")\n",
    "    print(f\"  Total images: {len(image_paths)}\")\n",
    "    print(f\"  Classes ({num_classes}): {class_names}\")\n",
    "    \n",
    "    label_counts = Counter(labels)\n",
    "    print(f\"\\n  Class distribution:\")\n",
    "    for cls_idx, cls_name in enumerate(class_names):\n",
    "        print(f\"    {cls_name}: {label_counts[cls_idx]}\")\n",
    "\n",
    "    # Stratified Split\n",
    "    print(f\"\\nCreating stratified split (val_size={CONFIG['val_size']})...\")\n",
    "    \n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=CONFIG['val_size'],\n",
    "                                random_state=CONFIG['random_seed'])\n",
    "    indices = np.arange(len(labels))\n",
    "    train_idx, val_idx = next(sss.split(indices, labels))\n",
    "\n",
    "    train_paths = [image_paths[i] for i in train_idx]\n",
    "    train_labels_list = [labels[i] for i in train_idx]\n",
    "    val_paths = [image_paths[i] for i in val_idx]\n",
    "    val_labels_list = [labels[i] for i in val_idx]\n",
    "\n",
    "    print(f\"  Train samples: {len(train_paths)}\")\n",
    "    print(f\"  Val samples: {len(val_paths)}\")\n",
    "\n",
    "    # Create Datasets\n",
    "    train_tf, val_tf = get_augmentation_transforms()\n",
    "    \n",
    "    train_dataset = PreCachedImageDataset(\n",
    "        train_paths, train_labels_list, \n",
    "        transform=train_tf, \n",
    "        cache_images=CONFIG['cache_images'],\n",
    "        img_size=CONFIG['img_size'],\n",
    "        is_train=True\n",
    "    )\n",
    "    val_dataset = PreCachedImageDataset(\n",
    "        val_paths, val_labels_list,\n",
    "        transform=None,\n",
    "        cache_images=CONFIG['cache_images'],\n",
    "        img_size=CONFIG['img_size'],\n",
    "        is_train=False\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "    )\n",
    "\n",
    "    # ============= Build Enhanced Model =============\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Building enhanced model...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    model = build_model(\n",
    "        model_name=CONFIG['model_name'],\n",
    "        num_classes=num_classes,\n",
    "        dropout_rate=CONFIG['dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"  Model: {CONFIG['model_name']}\")\n",
    "    print(f\"  Architecture: {CONFIG['model_name'].upper()} + 3-layer classifier\")\n",
    "    print(f\"  Trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "    # ============= Loss with Label Smoothing =============\n",
    "    if CONFIG['use_class_weights']:\n",
    "        train_counts = np.bincount(train_labels_list, minlength=num_classes)\n",
    "        class_weights = 1.0 / (train_counts + 1e-6)\n",
    "        class_weights = class_weights * (len(train_labels_list) / class_weights.sum())\n",
    "        class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "        print(f\"\\n  Class weights: {class_weights.cpu().numpy()}\")\n",
    "    else:\n",
    "        class_weights = None\n",
    "    \n",
    "    if CONFIG['use_label_smoothing']:\n",
    "        criterion = LabelSmoothingCrossEntropy(\n",
    "            epsilon=CONFIG['label_smooth_eps'], \n",
    "            weight=class_weights\n",
    "        )\n",
    "        print(f\"  Loss: Label Smoothing CE (eps={CONFIG['label_smooth_eps']})\")\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        print(f\"  Loss: Standard Cross Entropy\")\n",
    "\n",
    "    # ============= Training =============\n",
    "    best_val_acc, best_epoch = train_advanced(\n",
    "        model, train_loader, val_loader, criterion, device, CONFIG, class_names\n",
    "    )\n",
    "\n",
    "    # ============= Final Results =============\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Training Complete!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"  Best Val Acc: {best_val_acc:.2f}% (Epoch {best_epoch})\")\n",
    "    print(f\"  Target Range: 75-80%\")\n",
    "    if best_val_acc >= 75:\n",
    "        print(f\"  ✓ TARGET ACHIEVED!\")\n",
    "    else:\n",
    "        print(f\"  Gap to target: {75 - best_val_acc:.2f}%\")\n",
    "    print(f\"  Model saved to: {CONFIG['save_dir']}/best_model.pth\")\n",
    "\n",
    "    # ============= Final Results =============\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Training Complete!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"  Best Val Acc: {best_val_acc:.2f}% (Epoch {best_epoch})\")\n",
    "    print(f\"  Target Range: 75-80%\")\n",
    "    if best_val_acc >= 75:\n",
    "        print(f\"  ✓ TARGET ACHIEVED!\")\n",
    "    else:\n",
    "        print(f\"  Gap to target: {75 - best_val_acc:.2f}%\")\n",
    "    print(f\"  Model saved to: {CONFIG['save_dir']}/best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a228b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "\n",
      "============================================================\n",
      "Loading dataset...\n",
      "============================================================\n",
      "\n",
      "Dataset Statistics:\n",
      "  Total images: 35887\n",
      "  Classes (7): ['angry', 'disgusted', 'fearful', 'happy', 'neutral', 'sad', 'surprised']\n",
      "\n",
      "  Class distribution:\n",
      "    angry: 4953\n",
      "    disgusted: 547\n",
      "    fearful: 5121\n",
      "    happy: 8989\n",
      "    neutral: 6198\n",
      "    sad: 6077\n",
      "    surprised: 4002\n",
      "\n",
      "Creating stratified split (val_size=0.15)...\n",
      "  Train samples: 30503\n",
      "  Val samples: 5384\n",
      "Pre-loading 30503 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching: 100%|██████████| 30503/30503 [06:52<00:00, 74.01it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-loading 5384 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching: 100%|██████████| 5384/5384 [01:26<00:00, 61.97it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Building enhanced model...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Geomatex-LOQ\\AppData\\Local\\Temp\\ipykernel_31216\\1214035749.py:449: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler() if use_amp else None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Model: vit_b_16\n",
      "  Architecture: VIT_B_16 + 3-layer classifier\n",
      "  Trainable params: 87,119,111\n",
      "\n",
      "  Class weights: [ 2114.7783 19146.703   2045.306   1165.3425  1690.0564  1723.7594\n",
      "  2617.0537]\n",
      "  Loss: Label Smoothing CE (eps=0.1)\n",
      "\n",
      "============================================================\n",
      "Starting high-accuracy training with ViT backbone...\n",
      "Mixed Precision: ENABLED\n",
      "Target: 75-80% validation accuracy\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]:   0%|          | 0/477 [00:00<?, ?it/s]C:\\Users\\Geomatex-LOQ\\AppData\\Local\\Temp\\ipykernel_31216\\1214035749.py:310: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 1 [Val]:   0%|          | 0/85 [00:00<?, ?it/s]                                         C:\\Users\\Geomatex-LOQ\\AppData\\Local\\Temp\\ipykernel_31216\\1214035749.py:392: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "                                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 1/60]\n",
      "  Train -> Loss: 4605.6379  Acc: 8.67%\n",
      "  Val   -> Loss: 4496.5427  Acc: 10.72%\n",
      "  Gap: -2.04% | LR: 1.20e-04\n",
      "  ✓ New best! Val Acc: 10.72% (Gap: -2.04%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 2/60]\n",
      "  Train -> Loss: 4503.4270  Acc: 6.37%\n",
      "  Val   -> Loss: 4387.7775  Acc: 2.84%\n",
      "  Gap: 3.53% | LR: 1.20e-04\n",
      "  No improvement (1/12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 3/60]\n",
      "  Train -> Loss: 4457.7111  Acc: 4.27%\n",
      "  Val   -> Loss: 4379.5167  Acc: 2.62%\n",
      "  Gap: 1.65% | LR: 1.20e-04\n",
      "  No improvement (2/12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 4/60]\n",
      "  Train -> Loss: 4431.1481  Acc: 3.55%\n",
      "  Val   -> Loss: 4365.7392  Acc: 2.56%\n",
      "  Gap: 0.99% | LR: 1.20e-04\n",
      "  No improvement (3/12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 5/60]\n",
      "  Train -> Loss: 4417.5211  Acc: 3.52%\n",
      "  Val   -> Loss: 4357.0949  Acc: 2.17%\n",
      "  Gap: 1.35% | LR: 1.20e-04\n",
      "  No improvement (4/12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 6/60]\n",
      "  Train -> Loss: 4401.2205  Acc: 3.30%\n",
      "  Val   -> Loss: 4348.1410  Acc: 2.17%\n",
      "  Gap: 1.12% | LR: 1.20e-04\n",
      "  No improvement (5/12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 7/60]\n",
      "  Train -> Loss: 4396.5623  Acc: 3.95%\n",
      "  Val   -> Loss: 4337.5629  Acc: 5.83%\n",
      "  Gap: -1.89% | LR: 1.20e-04\n",
      "  No improvement (6/12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 8/60]\n",
      "  Train -> Loss: 4382.5975  Acc: 4.70%\n",
      "  Val   -> Loss: 4314.1113  Acc: 5.57%\n",
      "  Gap: -0.88% | LR: 1.20e-04\n",
      "  No improvement (7/12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 9/60]\n",
      "  Train -> Loss: 4371.5855  Acc: 5.56%\n",
      "  Val   -> Loss: 4288.7819  Acc: 5.03%\n",
      "  Gap: 0.53% | LR: 1.20e-04\n",
      "  No improvement (8/12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 10/60]\n",
      "  Train -> Loss: 4360.6124  Acc: 5.42%\n",
      "  Val   -> Loss: 4293.1100  Acc: 6.67%\n",
      "  Gap: -1.25% | LR: 1.20e-04\n",
      "  No improvement (9/12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 11/60]\n",
      "  Train -> Loss: 4347.8442  Acc: 6.41%\n",
      "  Val   -> Loss: 4279.2679  Acc: 6.32%\n",
      "  Gap: 0.10% | LR: 1.20e-04\n",
      "  No improvement (10/12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 12/60]\n",
      "  Train -> Loss: 4334.2594  Acc: 6.72%\n",
      "  Val   -> Loss: 4231.1365  Acc: 8.92%\n",
      "  Gap: -2.19% | LR: 1.20e-04\n",
      "  No improvement (11/12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 13/60]\n",
      "  Train -> Loss: 4313.6705  Acc: 8.57%\n",
      "  Val   -> Loss: 4224.6777  Acc: 16.03%\n",
      "  Gap: -7.46% | LR: 1.20e-04\n",
      "  ✓ New best! Val Acc: 16.03% (Gap: -7.46%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 14/60]\n",
      "  Train -> Loss: 4311.7160  Acc: 8.90%\n",
      "  Val   -> Loss: 4186.8806  Acc: 12.63%\n",
      "  Gap: -3.73% | LR: 1.20e-04\n",
      "  No improvement (1/12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 15/60]\n",
      "  Train -> Loss: 4299.1905  Acc: 10.24%\n",
      "  Val   -> Loss: 4177.6210  Acc: 16.53%\n",
      "  Gap: -6.29% | LR: 1.20e-04\n",
      "  ✓ New best! Val Acc: 16.53% (Gap: -6.29%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 16/60]\n",
      "  Train -> Loss: 4275.7074  Acc: 11.36%\n",
      "  Val   -> Loss: 4200.9513  Acc: 16.29%\n",
      "  Gap: -4.93% | LR: 1.20e-04\n",
      "  No improvement (1/12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 17/60]\n",
      "  Train -> Loss: 4267.6626  Acc: 12.81%\n",
      "  Val   -> Loss: 4145.5176  Acc: 15.17%\n",
      "  Gap: -2.36% | LR: 1.20e-04\n",
      "  No improvement (2/12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 18/60]\n",
      "  Train -> Loss: 4253.3246  Acc: 13.99%\n",
      "  Val   -> Loss: 4113.1862  Acc: 18.52%\n",
      "  Gap: -4.52% | LR: 1.20e-04\n",
      "  ✓ New best! Val Acc: 18.52% (Gap: -4.52%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 19/60]\n",
      "  Train -> Loss: 4224.6999  Acc: 15.92%\n",
      "  Val   -> Loss: 4130.6442  Acc: 29.42%\n",
      "  Gap: -13.50% | LR: 1.20e-04\n",
      "  ✓ New best! Val Acc: 29.42% (Gap: -13.50%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 20/60]\n",
      "  Train -> Loss: 4203.1476  Acc: 18.09%\n",
      "  Val   -> Loss: 4054.3557  Acc: 24.76%\n",
      "  Gap: -6.67% | LR: 1.20e-04\n",
      "  No improvement (1/12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 21/60]\n",
      "  Train -> Loss: 4192.2859  Acc: 18.31%\n",
      "  Val   -> Loss: 4055.3577  Acc: 26.76%\n",
      "  Gap: -8.46% | LR: 1.20e-04\n",
      "  TTA: Enabled\n",
      "  No improvement (2/12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 22/60]\n",
      "  Train -> Loss: 4165.4546  Acc: 20.52%\n",
      "  Val   -> Loss: 3980.4496  Acc: 30.81%\n",
      "  Gap: -10.29% | LR: 1.20e-04\n",
      "  TTA: Enabled\n",
      "  ✓ New best! Val Acc: 30.81% (Gap: -10.29%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 23/60]\n",
      "  Train -> Loss: 4161.5373  Acc: 21.00%\n",
      "  Val   -> Loss: 3938.8116  Acc: 27.90%\n",
      "  Gap: -6.90% | LR: 1.20e-04\n",
      "  TTA: Enabled\n",
      "  No improvement (1/12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 24/60]\n",
      "  Train -> Loss: 4125.8332  Acc: 22.81%\n",
      "  Val   -> Loss: 3850.7500  Acc: 31.91%\n",
      "  Gap: -9.10% | LR: 1.20e-04\n",
      "  TTA: Enabled\n",
      "  ✓ New best! Val Acc: 31.91% (Gap: -9.10%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 25/60]\n",
      "  Train -> Loss: 4126.4679  Acc: 23.96%\n",
      "  Val   -> Loss: 3870.8091  Acc: 25.48%\n",
      "  Gap: -1.52% | LR: 1.20e-04\n",
      "  TTA: Enabled\n",
      "  No improvement (1/12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 26/60]\n",
      "  Train -> Loss: 4117.1874  Acc: 23.85%\n",
      "  Val   -> Loss: 3884.8939  Acc: 33.66%\n",
      "  Gap: -9.80% | LR: 1.20e-04\n",
      "  TTA: Enabled\n",
      "  ✓ New best! Val Acc: 33.66% (Gap: -9.80%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 27/60]\n",
      "  Train -> Loss: 4103.3701  Acc: 24.69%\n",
      "  Val   -> Loss: 3856.3130  Acc: 27.88%\n",
      "  Gap: -3.19% | LR: 1.20e-04\n",
      "  TTA: Enabled\n",
      "  No improvement (1/12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 28/60]\n",
      "  Train -> Loss: 4082.9649  Acc: 25.93%\n",
      "  Val   -> Loss: 3829.5598  Acc: 31.80%\n",
      "  Gap: -5.86% | LR: 1.20e-04\n",
      "  TTA: Enabled\n",
      "  No improvement (2/12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 29/60]\n",
      "  Train -> Loss: 4053.1219  Acc: 27.95%\n",
      "  Val   -> Loss: 3794.0193  Acc: 38.63%\n",
      "  Gap: -10.68% | LR: 1.20e-04\n",
      "  TTA: Enabled\n",
      "  ✓ New best! Val Acc: 38.63% (Gap: -10.68%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 30/60]\n",
      "  Train -> Loss: 4045.8313  Acc: 27.74%\n",
      "  Val   -> Loss: 3783.8509  Acc: 36.00%\n",
      "  Gap: -8.26% | LR: 1.20e-04\n",
      "  TTA: Enabled\n",
      "  No improvement (1/12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 31/60]\n",
      "  Train -> Loss: 4021.7997  Acc: 29.57%\n",
      "  Val   -> Loss: 3733.9746  Acc: 38.61%\n",
      "  Gap: -9.04% | LR: 1.20e-04\n",
      "  TTA: Enabled\n",
      "  No improvement (2/12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 32/60]\n",
      "  Train -> Loss: 4018.5041  Acc: 29.52%\n",
      "  Val   -> Loss: 3719.6648  Acc: 33.62%\n",
      "  Gap: -4.10% | LR: 1.20e-04\n",
      "  TTA: Enabled\n",
      "  No improvement (3/12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 33/60]\n",
      "  Train -> Loss: 4014.0926  Acc: 30.03%\n",
      "  Val   -> Loss: 3693.6085  Acc: 40.55%\n",
      "  Gap: -10.52% | LR: 1.20e-04\n",
      "  TTA: Enabled\n",
      "  ✓ New best! Val Acc: 40.55% (Gap: -10.52%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 34/60]\n",
      "  Train -> Loss: 4012.9070  Acc: 30.01%\n",
      "  Val   -> Loss: 3788.8727  Acc: 36.33%\n",
      "  Gap: -6.32% | LR: 1.20e-04\n",
      "  TTA: Enabled\n",
      "  No improvement (1/12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 35/60]\n",
      "  Train -> Loss: 3995.5719  Acc: 30.09%\n",
      "  Val   -> Loss: 3737.4357  Acc: 39.60%\n",
      "  Gap: -9.50% | LR: 1.20e-04\n",
      "  TTA: Enabled\n",
      "  No improvement (2/12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 36/60]\n",
      "  Train -> Loss: 4018.7474  Acc: 30.04%\n",
      "  Val   -> Loss: 3657.9405  Acc: 41.81%\n",
      "  Gap: -11.77% | LR: 1.20e-04\n",
      "  TTA: Enabled\n",
      "  ✓ New best! Val Acc: 41.81% (Gap: -11.77%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 37/60]\n",
      "  Train -> Loss: 3978.4998  Acc: 31.00%\n",
      "  Val   -> Loss: 3671.2495  Acc: 41.16%\n",
      "  Gap: -10.16% | LR: 1.20e-04\n",
      "  TTA: Enabled\n",
      "  No improvement (1/12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 38/60]\n",
      "  Train -> Loss: 3946.1577  Acc: 32.80%\n",
      "  Val   -> Loss: 3618.1123  Acc: 46.94%\n",
      "  Gap: -14.14% | LR: 1.20e-04\n",
      "  TTA: Enabled\n",
      "  ✓ New best! Val Acc: 46.94% (Gap: -14.14%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 668\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    665\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Loss: Standard Cross Entropy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    667\u001b[0m \u001b[38;5;66;03m# ============= Training =============\u001b[39;00m\n\u001b[1;32m--> 668\u001b[0m best_val_acc, best_epoch \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_advanced\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_names\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;66;03m# ============= Final Results =============\u001b[39;00m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 465\u001b[0m, in \u001b[0;36mtrain_advanced\u001b[1;34m(model, train_loader, val_loader, criterion, device, CONFIG, class_names)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, CONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 465\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_mixup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muse_mixup\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmixup_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmixup_alpha\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_amp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_amp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cutmix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muse_cutmix\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    472\u001b[0m     use_tta \u001b[38;5;241m=\u001b[39m CONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_tta\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m epoch \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[0;32m    473\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m evaluate(\n\u001b[0;32m    474\u001b[0m         model, val_loader, criterion, device, epoch, \n\u001b[0;32m    475\u001b[0m         use_amp\u001b[38;5;241m=\u001b[39muse_amp, use_tta\u001b[38;5;241m=\u001b[39muse_tta\n\u001b[0;32m    476\u001b[0m     )\n",
      "Cell \u001b[1;32mIn[3], line 302\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, loader, criterion, optimizer, device, epoch, scaler, use_mixup, mixup_alpha, use_amp, use_cutmix)\u001b[0m\n\u001b[0;32m    298\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    300\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m [Train]\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 302\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[0;32m    303\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    304\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\adam\\AMIT_Diploma\\grad_project\\FER\\.venv_gradproj\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\adam\\AMIT_Diploma\\grad_project\\FER\\.venv_gradproj\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    739\u001b[0m ):\n",
      "File \u001b[1;32mc:\\adam\\AMIT_Diploma\\grad_project\\FER\\.venv_gradproj\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\adam\\AMIT_Diploma\\grad_project\\FER\\.venv_gradproj\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\adam\\AMIT_Diploma\\grad_project\\FER\\.venv_gradproj\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[3], line 192\u001b[0m, in \u001b[0;36mPreCachedImageDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    190\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcached_tensors[idx]\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_train \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m--> 192\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    194\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_paths[idx])\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\adam\\AMIT_Diploma\\grad_project\\FER\\.venv_gradproj\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\adam\\AMIT_Diploma\\grad_project\\FER\\.venv_gradproj\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\adam\\AMIT_Diploma\\grad_project\\FER\\.venv_gradproj\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\adam\\AMIT_Diploma\\grad_project\\FER\\.venv_gradproj\\lib\\site-packages\\torchvision\\transforms\\transforms.py:805\u001b[0m, in \u001b[0;36mRandomPerspective.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp:\n\u001b[0;32m    804\u001b[0m     startpoints, endpoints \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params(width, height, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistortion_scale)\n\u001b[1;32m--> 805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperspective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstartpoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\adam\\AMIT_Diploma\\grad_project\\FER\\.venv_gradproj\\lib\\site-packages\\torchvision\\transforms\\functional.py:754\u001b[0m, in \u001b[0;36mperspective\u001b[1;34m(img, startpoints, endpoints, interpolation, fill)\u001b[0m\n\u001b[0;32m    751\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F_pil\u001b[38;5;241m.\u001b[39mperspective(img, coeffs, interpolation\u001b[38;5;241m=\u001b[39mpil_interpolation, fill\u001b[38;5;241m=\u001b[39mfill)\n\u001b[1;32m--> 754\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperspective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoeffs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\adam\\AMIT_Diploma\\grad_project\\FER\\.venv_gradproj\\lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:724\u001b[0m, in \u001b[0;36mperspective\u001b[1;34m(img, perspective_coeffs, interpolation, fill)\u001b[0m\n\u001b[0;32m    722\u001b[0m dtype \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_floating_point(img) \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[0;32m    723\u001b[0m grid \u001b[38;5;241m=\u001b[39m _perspective_grid(perspective_coeffs, ow\u001b[38;5;241m=\u001b[39mow, oh\u001b[38;5;241m=\u001b[39moh, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mimg\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 724\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_apply_grid_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\adam\\AMIT_Diploma\\grad_project\\FER\\.venv_gradproj\\lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:558\u001b[0m, in \u001b[0;36m_apply_grid_transform\u001b[1;34m(img, grid, mode, fill)\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fill \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    557\u001b[0m     mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m, img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m]), dtype\u001b[38;5;241m=\u001b[39mimg\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mimg\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 558\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    560\u001b[0m img \u001b[38;5;241m=\u001b[39m grid_sample(img, grid, mode\u001b[38;5;241m=\u001b[39mmode, padding_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    562\u001b[0m \u001b[38;5;66;03m# Fill with required color\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a849e12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_gradproj (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
