{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36bf21a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "============================================================\n",
      "Loading dataset...\n",
      "============================================================\n",
      "\n",
      "Dataset Statistics:\n",
      "  Total images: 35887\n",
      "  Classes (7): ['angry', 'disgusted', 'fearful', 'happy', 'neutral', 'sad', 'surprised']\n",
      "\n",
      "  Class distribution:\n",
      "    angry: 4953\n",
      "    disgusted: 547\n",
      "    fearful: 5121\n",
      "    happy: 8989\n",
      "    neutral: 6198\n",
      "    sad: 6077\n",
      "    surprised: 4002\n",
      "\n",
      "Creating stratified split (val_size=0.15)...\n",
      "  Train samples: 30503\n",
      "  Val samples: 5384\n",
      "Pre-loading and processing 30503 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching: 100%|██████████| 30503/30503 [04:12<00:00, 120.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-loading and processing 5384 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching: 100%|██████████| 5384/5384 [00:52<00:00, 103.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Building model...\n",
      "============================================================\n",
      "  Model: resnet34\n",
      "  Freeze backbone: False\n",
      "  Dropout: 0.6\n",
      "  Trainable params: 21,681,991\n",
      "\n",
      "  Using class weights: [ 2114.7783 19146.703   2045.306   1165.3425  1690.0564  1723.7594\n",
      "  2617.0537]\n",
      "  Using Focal Loss (gamma=2.0)\n",
      "\n",
      "============================================================\n",
      "Starting training...\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "PHASE 1: Training classifier head only (5 epochs)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 1]\n",
      "  Train -> Loss: 4002.4163  Acc: 19.48%\n",
      "  Val   -> Loss: 3621.9444  Acc: 32.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 2]\n",
      "  Train -> Loss: 3762.4457  Acc: 25.27%\n",
      "  Val   -> Loss: 3524.4703  Acc: 34.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 3]\n",
      "  Train -> Loss: 3683.6070  Acc: 27.02%\n",
      "  Val   -> Loss: 3476.5845  Acc: 35.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 4]\n",
      "  Train -> Loss: 3625.6778  Acc: 28.73%\n",
      "  Val   -> Loss: 3473.3585  Acc: 35.22%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 5]\n",
      "  Train -> Loss: 3627.0427  Acc: 28.13%\n",
      "  Val   -> Loss: 3466.8112  Acc: 35.46%\n",
      "\n",
      "============================================================\n",
      "PHASE 2: Unfreezing last backbone layer (10 epochs)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 6]\n",
      "  Train -> Loss: 3502.7510  Acc: 31.80%\n",
      "  Val   -> Loss: 3143.8893  Acc: 39.86%\n",
      "  ✓ New best model saved! (Val Acc: 39.86%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 7]\n",
      "  Train -> Loss: 3295.5076  Acc: 36.81%\n",
      "  Val   -> Loss: 2988.2536  Acc: 43.09%\n",
      "  ✓ New best model saved! (Val Acc: 43.09%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 8]\n",
      "  Train -> Loss: 3104.2231  Acc: 41.42%\n",
      "  Val   -> Loss: 2835.7547  Acc: 44.76%\n",
      "  ✓ New best model saved! (Val Acc: 44.76%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 9]\n",
      "  Train -> Loss: 3033.1628  Acc: 42.60%\n",
      "  Val   -> Loss: 2737.3676  Acc: 47.49%\n",
      "  ✓ New best model saved! (Val Acc: 47.49%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 10]\n",
      "  Train -> Loss: 2973.5622  Acc: 44.08%\n",
      "  Val   -> Loss: 2679.6961  Acc: 48.12%\n",
      "  ✓ New best model saved! (Val Acc: 48.12%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 11]\n",
      "  Train -> Loss: 2932.4476  Acc: 44.48%\n",
      "  Val   -> Loss: 2634.4183  Acc: 49.55%\n",
      "  ✓ New best model saved! (Val Acc: 49.55%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 12]\n",
      "  Train -> Loss: 2895.4007  Acc: 45.35%\n",
      "  Val   -> Loss: 2589.8231  Acc: 50.15%\n",
      "  ✓ New best model saved! (Val Acc: 50.15%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 13]\n",
      "  Train -> Loss: 2887.1766  Acc: 45.96%\n",
      "  Val   -> Loss: 2563.7633  Acc: 50.69%\n",
      "  ✓ New best model saved! (Val Acc: 50.69%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 14]\n",
      "  Train -> Loss: 2788.6740  Acc: 47.48%\n",
      "  Val   -> Loss: 2558.8763  Acc: 51.30%\n",
      "  ✓ New best model saved! (Val Acc: 51.30%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 15]\n",
      "  Train -> Loss: 2815.8385  Acc: 46.70%\n",
      "  Val   -> Loss: 2552.6190  Acc: 51.71%\n",
      "  ✓ New best model saved! (Val Acc: 51.71%)\n",
      "\n",
      "============================================================\n",
      "PHASE 3: Fine-tuning entire model\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 16/50]\n",
      "  Train -> Loss: 2794.2076  Acc: 47.94%\n",
      "  Val   -> Loss: 2407.2987  Acc: 54.81%\n",
      "  ✓ New best model saved! (Val Acc: 54.81%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 17/50]\n",
      "  Train -> Loss: 2683.9378  Acc: 49.92%\n",
      "  Val   -> Loss: 2319.0945  Acc: 55.98%\n",
      "  ✓ New best model saved! (Val Acc: 55.98%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 18/50]\n",
      "  Train -> Loss: 2565.8418  Acc: 52.10%\n",
      "  Val   -> Loss: 2270.4415  Acc: 57.00%\n",
      "  ✓ New best model saved! (Val Acc: 57.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 19/50]\n",
      "  Train -> Loss: 2543.6347  Acc: 52.86%\n",
      "  Val   -> Loss: 2215.6637  Acc: 59.14%\n",
      "  ✓ New best model saved! (Val Acc: 59.14%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 20/50]\n",
      "  Train -> Loss: 2448.9852  Acc: 54.55%\n",
      "  Val   -> Loss: 2186.3733  Acc: 60.55%\n",
      "  ✓ New best model saved! (Val Acc: 60.55%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 21/50]\n",
      "  Train -> Loss: 2447.2880  Acc: 54.96%\n",
      "  Val   -> Loss: 2148.1071  Acc: 60.42%\n",
      "  No improvement (1/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 22/50]\n",
      "  Train -> Loss: 2390.7283  Acc: 56.22%\n",
      "  Val   -> Loss: 2111.4360  Acc: 61.81%\n",
      "  ✓ New best model saved! (Val Acc: 61.81%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 23/50]\n",
      "  Train -> Loss: 2268.4016  Acc: 58.33%\n",
      "  Val   -> Loss: 2094.2182  Acc: 62.80%\n",
      "  ✓ New best model saved! (Val Acc: 62.80%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 24/50]\n",
      "  Train -> Loss: 2276.3571  Acc: 57.94%\n",
      "  Val   -> Loss: 2052.1774  Acc: 62.04%\n",
      "  No improvement (1/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 25/50]\n",
      "  Train -> Loss: 2265.5541  Acc: 58.53%\n",
      "  Val   -> Loss: 2024.9540  Acc: 63.22%\n",
      "  ✓ New best model saved! (Val Acc: 63.22%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 26/50]\n",
      "  Train -> Loss: 2288.4706  Acc: 58.41%\n",
      "  Val   -> Loss: 2024.9293  Acc: 63.19%\n",
      "  No improvement (1/10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 27/50]\n",
      "  Train -> Loss: 2242.7141  Acc: 59.18%\n",
      "  Val   -> Loss: 1973.0883  Acc: 63.87%\n",
      "  ✓ New best model saved! (Val Acc: 63.87%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 696\u001b[39m\n\u001b[32m    692\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Model saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG[\u001b[33m'\u001b[39m\u001b[33msave_dir\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/best_model.pth\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    695\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m696\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 680\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    677\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m    679\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m CONFIG[\u001b[33m'\u001b[39m\u001b[33muse_gradual_unfreezing\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m680\u001b[39m     best_val_acc, best_epoch = \u001b[43mtrain_with_gradual_unfreezing\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_names\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    684\u001b[39m     \u001b[38;5;66;03m# Standard training loop would go here\u001b[39;00m\n\u001b[32m    685\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 482\u001b[39m, in \u001b[36mtrain_with_gradual_unfreezing\u001b[39m\u001b[34m(model, train_loader, val_loader, criterion, device, CONFIG, class_names)\u001b[39m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, remaining_epochs + \u001b[32m1\u001b[39m):\n\u001b[32m    481\u001b[39m     current_epoch += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m     train_loss, train_acc = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_mixup\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43muse_mixup\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmixup_alpha\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmixup_alpha\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m     val_loss, val_acc = evaluate(\n\u001b[32m    487\u001b[39m         model, val_loader, criterion, device, current_epoch,\n\u001b[32m    488\u001b[39m         use_tta=CONFIG[\u001b[33m'\u001b[39m\u001b[33muse_tta\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    489\u001b[39m     )\n\u001b[32m    491\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG[\u001b[33m'\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 311\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, loader, criterion, optimizer, device, epoch, use_mixup, mixup_alpha)\u001b[39m\n\u001b[32m    309\u001b[39m outputs = model(images)\n\u001b[32m    310\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    312\u001b[39m optimizer.step()\n\u001b[32m    314\u001b[39m running_loss += loss.item() * images.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\adam\\AMIT_Diploma\\grad_project\\.venv_grad\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\adam\\AMIT_Diploma\\grad_project\\.venv_grad\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\adam\\AMIT_Diploma\\grad_project\\.venv_grad\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Improved FER Training Pipeline with:\n",
    " - Enhanced regularization (higher dropout, weight decay)\n",
    " - Advanced data augmentation (rotation, color jitter, mixup)\n",
    " - Focal loss for class imbalance\n",
    " - Gradual unfreezing strategy\n",
    " - Better learning rate schedule\n",
    " - Test-Time Augmentation support\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms, models\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  1. Focal Loss for Imbalanced Data\n",
    "# ============================================================\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Loss for handling class imbalance.\n",
    "    Focuses training on hard examples.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma * ce_loss)\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  2. Mixup Data Augmentation\n",
    "# ============================================================\n",
    "\n",
    "def mixup_data(x, y, alpha=0.2, device='cuda'):\n",
    "    \"\"\"\n",
    "    Mixup augmentation: interpolates between two samples.\n",
    "    Helps improve generalization.\n",
    "    \"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  3. Data Collection\n",
    "# ============================================================\n",
    "\n",
    "def gather_image_paths_and_labels(root_dir: str) -> Tuple[List[str], List[int], List[str]]:\n",
    "    \"\"\"\n",
    "    Collect all images from emotion class subdirectories.\n",
    "    Handles both structures:\n",
    "      - root/emotion/*.jpg\n",
    "      - root/train/emotion/*.jpg and root/test/emotion/*.jpg\n",
    "    \n",
    "    Returns:\n",
    "        image_paths: List of file paths\n",
    "        labels: List of integer labels\n",
    "        class_names: List of class names (sorted)\n",
    "    \"\"\"\n",
    "    root = Path(root_dir)\n",
    "    if not root.exists():\n",
    "        raise FileNotFoundError(f\"Dataset root not found: {root_dir}\")\n",
    "\n",
    "    # Collect all images with their parent directory (emotion class)\n",
    "    image_data = []\n",
    "    exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\"}\n",
    "    \n",
    "    for img_path in root.rglob(\"*\"):\n",
    "        if img_path.suffix.lower() in exts:\n",
    "            # Get emotion class (parent directory name)\n",
    "            emotion_class = img_path.parent.name\n",
    "            # Skip if parent is 'train' or 'test' - go up one more level\n",
    "            if emotion_class in ['train', 'test']:\n",
    "                emotion_class = img_path.parent.parent.name\n",
    "            \n",
    "            image_data.append((str(img_path), emotion_class))\n",
    "    \n",
    "    if not image_data:\n",
    "        raise ValueError(f\"No images found under {root_dir}\")\n",
    "    \n",
    "    # Create class mapping\n",
    "    unique_classes = sorted(set(emotion for _, emotion in image_data))\n",
    "    class_to_idx = {name: idx for idx, name in enumerate(unique_classes)}\n",
    "    \n",
    "    # Convert to lists\n",
    "    image_paths = [path for path, _ in image_data]\n",
    "    labels = [class_to_idx[emotion] for _, emotion in image_data]\n",
    "    \n",
    "    return image_paths, labels, unique_classes\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  4. Pre-cached Dataset\n",
    "# ============================================================\n",
    "\n",
    "class PreCachedImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Ultra-fast dataset that stores pre-processed tensors in memory.\n",
    "    For training: applies augmentation on cached tensors.\n",
    "    For validation: returns tensors directly (no augmentation).\n",
    "    \"\"\"\n",
    "    def __init__(self, image_paths: List[str], labels: List[int], \n",
    "                 transform=None, cache_images: bool = True, img_size: int = 224,\n",
    "                 is_train: bool = True):\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "        self.cached_tensors = []\n",
    "        \n",
    "        if cache_images:\n",
    "            print(f\"Pre-loading and processing {len(image_paths)} images...\")\n",
    "            \n",
    "            # Pre-process transform (applied once during caching)\n",
    "            imagenet_mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "            imagenet_std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "            \n",
    "            resize_transform = transforms.Compose([\n",
    "                transforms.Resize(img_size),\n",
    "                transforms.ToTensor(),\n",
    "            ])\n",
    "            \n",
    "            for path in tqdm(image_paths, desc=\"Caching\"):\n",
    "                try:\n",
    "                    img = Image.open(path).convert(\"RGB\")\n",
    "                    img_tensor = resize_transform(img)\n",
    "                    \n",
    "                    # For validation, pre-normalize to avoid doing it on every access\n",
    "                    if not is_train:\n",
    "                        img_tensor = (img_tensor - imagenet_mean) / imagenet_std\n",
    "                    \n",
    "                    self.cached_tensors.append(img_tensor)\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Failed to load {path}: {e}\")\n",
    "                    blank = torch.zeros(3, img_size, img_size)\n",
    "                    if not is_train:\n",
    "                        blank = (blank - imagenet_mean) / imagenet_std\n",
    "                    self.cached_tensors.append(blank)\n",
    "        else:\n",
    "            self.cached_tensors = None\n",
    "            self.image_paths = image_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.cached_tensors is not None:\n",
    "            img = self.cached_tensors[idx]\n",
    "            \n",
    "            # For training, apply augmentation\n",
    "            if self.is_train and self.transform:\n",
    "                img = self.transform(img)\n",
    "            \n",
    "        else:\n",
    "            img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "        \n",
    "        return img, self.labels[idx]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  5. Enhanced Transforms\n",
    "# ============================================================\n",
    "\n",
    "def get_augmentation_transforms() -> Tuple[transforms.Compose, transforms.Compose]:\n",
    "    \"\"\"\n",
    "    Returns enhanced augmentation transforms for better generalization.\n",
    "    \"\"\"\n",
    "    imagenet_mean = [0.485, 0.456, 0.406]\n",
    "    imagenet_std = [0.229, 0.224, 0.225]\n",
    "    \n",
    "    # Train: aggressive augmentation\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(15),  # Add rotation\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Color variation\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),  # Translation & scale\n",
    "        transforms.Normalize(imagenet_mean, imagenet_std),\n",
    "        transforms.RandomErasing(p=0.2, scale=(0.02, 0.15)),  # Random erasing\n",
    "    ])\n",
    "    \n",
    "    # Val: just normalize\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Normalize(imagenet_mean, imagenet_std),\n",
    "    ])\n",
    "    \n",
    "    return train_transform, val_transform\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  6. Improved Model Builder\n",
    "# ============================================================\n",
    "\n",
    "def build_model(model_name: str = 'resnet34', num_classes: int = 7, \n",
    "                dropout_rate: float = 0.6, freeze_backbone: bool = False) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Build ResNet-based classifier with enhanced regularization.\n",
    "    \"\"\"\n",
    "    if model_name == 'resnet18':\n",
    "        weights = models.ResNet18_Weights.IMAGENET1K_V1\n",
    "        model = models.resnet18(weights=weights)\n",
    "    elif model_name == 'resnet34':\n",
    "        weights = models.ResNet34_Weights.IMAGENET1K_V1\n",
    "        model = models.resnet34(weights=weights)\n",
    "    elif model_name == 'resnet50':\n",
    "        weights = models.ResNet50_Weights.IMAGENET1K_V1\n",
    "        model = models.resnet50(weights=weights)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "\n",
    "    # Replace classifier head with higher dropout\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(in_features, 512),\n",
    "        nn.BatchNorm1d(512),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(dropout_rate),  # Increased dropout\n",
    "        nn.Linear(512, 256),\n",
    "        nn.BatchNorm1d(256),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(dropout_rate),  # Increased dropout\n",
    "        nn.Linear(256, num_classes)\n",
    "    )\n",
    "\n",
    "    # Freeze backbone if requested\n",
    "    if freeze_backbone:\n",
    "        for name, param in model.named_parameters():\n",
    "            if \"fc\" not in name:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  7. Training Functions with Mixup\n",
    "# ============================================================\n",
    "\n",
    "def train_one_epoch(model: nn.Module, loader: DataLoader, criterion: nn.Module,\n",
    "                   optimizer: optim.Optimizer, device: torch.device, epoch: int,\n",
    "                   use_mixup: bool = True, mixup_alpha: float = 0.2) -> Tuple[float, float]:\n",
    "    \"\"\"Train for one epoch with optional mixup augmentation\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch} [Train]\", leave=False)\n",
    "\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        # Apply mixup augmentation\n",
    "        if use_mixup and random.random() < 0.5:  # Apply mixup 50% of the time\n",
    "            mixed_images, labels_a, labels_b, lam = mixup_data(images, labels, mixup_alpha, device)\n",
    "            \n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            outputs = model(mixed_images)\n",
    "            loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # For accuracy, use original labels\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (lam * (preds == labels_a).sum().item() + (1 - lam) * (preds == labels_b).sum().item())\n",
    "            total += images.size(0)\n",
    "        else:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += images.size(0)\n",
    "\n",
    "        # Update progress bar\n",
    "        if total % (images.size(0) * 5) == 0:\n",
    "            avg_loss = running_loss / total\n",
    "            avg_acc = 100.0 * correct / total\n",
    "            pbar.set_postfix({\"loss\": f\"{avg_loss:.4f}\", \"acc\": f\"{avg_acc:.2f}%\"})\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    avg_acc = 100.0 * correct / total\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module, loader: DataLoader, criterion: nn.Module,\n",
    "            device: torch.device, epoch: int, use_tta: bool = False) -> Tuple[float, float]:\n",
    "    \"\"\"Evaluate on validation set with optional Test-Time Augmentation\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch} [Val]\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in pbar:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            if use_tta:\n",
    "                # Test-Time Augmentation: average predictions over augmented versions\n",
    "                outputs_list = []\n",
    "                outputs_list.append(model(images))\n",
    "                \n",
    "                # Horizontal flip\n",
    "                outputs_list.append(model(torch.flip(images, dims=[3])))\n",
    "                \n",
    "                outputs = torch.stack(outputs_list).mean(dim=0)\n",
    "            else:\n",
    "                outputs = model(images)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += images.size(0)\n",
    "\n",
    "            # Update progress bar\n",
    "            avg_loss = running_loss / total\n",
    "            avg_acc = 100.0 * correct / total\n",
    "            pbar.set_postfix({\"loss\": f\"{avg_loss:.4f}\", \"acc\": f\"{avg_acc:.2f}%\"})\n",
    "\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  8. Gradual Unfreezing Training\n",
    "# ============================================================\n",
    "\n",
    "def train_with_gradual_unfreezing(model, train_loader, val_loader, criterion, \n",
    "                                  device, CONFIG, class_names):\n",
    "    \"\"\"\n",
    "    Three-phase training:\n",
    "    1. Train only classifier head\n",
    "    2. Unfreeze last layer of backbone\n",
    "    3. Fine-tune entire model\n",
    "    \"\"\"\n",
    "    best_val_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    patience_counter = 0\n",
    "    current_epoch = 0\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PHASE 1: Training classifier head only (5 epochs)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Phase 1: Freeze backbone, train classifier\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.fc.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    optimizer = optim.AdamW(\n",
    "        model.fc.parameters(),\n",
    "        lr=CONFIG['lr'],\n",
    "        weight_decay=CONFIG['weight_decay']\n",
    "    )\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=5, eta_min=1e-6)\n",
    "    \n",
    "    for epoch in range(1, 6):\n",
    "        current_epoch += 1\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, device, current_epoch,\n",
    "            use_mixup=CONFIG['use_mixup'], mixup_alpha=CONFIG['mixup_alpha']\n",
    "        )\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device, current_epoch)\n",
    "        \n",
    "        print(f\"\\n[Epoch {current_epoch}]\")\n",
    "        print(f\"  Train -> Loss: {train_loss:.4f}  Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Val   -> Loss: {val_loss:.4f}  Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = current_epoch\n",
    "            save_checkpoint(model, optimizer, current_epoch, val_acc, val_loss, \n",
    "                          class_names, CONFIG)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PHASE 2: Unfreezing last backbone layer (10 epochs)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Phase 2: Unfreeze layer4\n",
    "    for param in model.layer4.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    optimizer = optim.AdamW([\n",
    "        {'params': model.layer4.parameters(), 'lr': CONFIG['lr'] / 10},\n",
    "        {'params': model.fc.parameters(), 'lr': CONFIG['lr'] / 2}\n",
    "    ], weight_decay=CONFIG['weight_decay'])\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6)\n",
    "    \n",
    "    for epoch in range(1, 11):\n",
    "        current_epoch += 1\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, device, current_epoch,\n",
    "            use_mixup=CONFIG['use_mixup'], mixup_alpha=CONFIG['mixup_alpha']\n",
    "        )\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device, current_epoch)\n",
    "        \n",
    "        print(f\"\\n[Epoch {current_epoch}]\")\n",
    "        print(f\"  Train -> Loss: {train_loss:.4f}  Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Val   -> Loss: {val_loss:.4f}  Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = current_epoch\n",
    "            patience_counter = 0\n",
    "            save_checkpoint(model, optimizer, current_epoch, val_acc, val_loss,\n",
    "                          class_names, CONFIG)\n",
    "            print(f\"  ✓ New best model saved! (Val Acc: {val_acc:.2f}%)\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PHASE 3: Fine-tuning entire model\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Phase 3: Unfreeze everything\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=CONFIG['lr'] / 20,\n",
    "        weight_decay=CONFIG['weight_decay']\n",
    "    )\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "    \n",
    "    remaining_epochs = CONFIG['epochs'] - current_epoch\n",
    "    for epoch in range(1, remaining_epochs + 1):\n",
    "        current_epoch += 1\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, device, current_epoch,\n",
    "            use_mixup=CONFIG['use_mixup'], mixup_alpha=CONFIG['mixup_alpha']\n",
    "        )\n",
    "        val_loss, val_acc = evaluate(\n",
    "            model, val_loader, criterion, device, current_epoch,\n",
    "            use_tta=CONFIG['use_tta']\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n[Epoch {current_epoch}/{CONFIG['epochs']}]\")\n",
    "        print(f\"  Train -> Loss: {train_loss:.4f}  Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Val   -> Loss: {val_loss:.4f}  Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = current_epoch\n",
    "            patience_counter = 0\n",
    "            save_checkpoint(model, optimizer, current_epoch, val_acc, val_loss,\n",
    "                          class_names, CONFIG)\n",
    "            print(f\"  ✓ New best model saved! (Val Acc: {val_acc:.2f}%)\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  No improvement ({patience_counter}/{CONFIG['patience']})\")\n",
    "            \n",
    "            if patience_counter >= CONFIG['patience']:\n",
    "                print(f\"\\nEarly stopping triggered\")\n",
    "                break\n",
    "    \n",
    "    return best_val_acc, best_epoch\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, val_acc, val_loss, class_names, CONFIG):\n",
    "    \"\"\"Save model checkpoint\"\"\"\n",
    "    best_path = os.path.join(CONFIG['save_dir'], \"best_model.pth\")\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'val_acc': val_acc,\n",
    "        'val_loss': val_loss,\n",
    "        'class_names': class_names,\n",
    "        'config': CONFIG\n",
    "    }, best_path)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  9. Main Training Pipeline\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    # ============= Configuration =============\n",
    "    CONFIG = {\n",
    "        'data_root': 'C:/adam/AMIT_Diploma/grad_project/archive (1)',\n",
    "        'model_name': 'resnet34',  # Changed to resnet34 for more capacity\n",
    "        'img_size': 224,\n",
    "        'batch_size': 256,\n",
    "        'epochs': 50,\n",
    "        'lr': 3e-4,  # Reduced initial LR\n",
    "        'weight_decay': 5e-4,  # Increased weight decay\n",
    "        'dropout': 0.6,  # Increased dropout\n",
    "        'freeze_backbone': False,\n",
    "        'val_size': 0.15,\n",
    "        'random_seed': 42,\n",
    "        'num_workers': 0,\n",
    "        'patience': 10,  # Increased patience\n",
    "        'use_class_weights': True,\n",
    "        'use_focal_loss': True,  # Use focal loss\n",
    "        'focal_gamma': 2.0,\n",
    "        'use_mixup': True,  # Use mixup augmentation\n",
    "        'mixup_alpha': 0.2,\n",
    "        'use_tta': True,  # Use test-time augmentation for validation\n",
    "        'use_gradual_unfreezing': True,  # Use gradual unfreezing strategy\n",
    "        'save_dir': './checkpoints',\n",
    "        'cache_images': True,\n",
    "    }\n",
    "\n",
    "    # ============= Setup =============\n",
    "    random.seed(CONFIG['random_seed'])\n",
    "    np.random.seed(CONFIG['random_seed'])\n",
    "    torch.manual_seed(CONFIG['random_seed'])\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(CONFIG['random_seed'])\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    os.makedirs(CONFIG['save_dir'], exist_ok=True)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # ============= Load Data =============\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Loading dataset...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    image_paths, labels, class_names = gather_image_paths_and_labels(CONFIG['data_root'])\n",
    "    num_classes = len(class_names)\n",
    "    \n",
    "    print(f\"\\nDataset Statistics:\")\n",
    "    print(f\"  Total images: {len(image_paths)}\")\n",
    "    print(f\"  Classes ({num_classes}): {class_names}\")\n",
    "    \n",
    "    # Class distribution\n",
    "    label_counts = Counter(labels)\n",
    "    print(f\"\\n  Class distribution:\")\n",
    "    for cls_idx, cls_name in enumerate(class_names):\n",
    "        print(f\"    {cls_name}: {label_counts[cls_idx]}\")\n",
    "\n",
    "    # ============= Stratified Split =============\n",
    "    print(f\"\\nCreating stratified split (val_size={CONFIG['val_size']})...\")\n",
    "    \n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=CONFIG['val_size'],\n",
    "                                random_state=CONFIG['random_seed'])\n",
    "    indices = np.arange(len(labels))\n",
    "    train_idx, val_idx = next(sss.split(indices, labels))\n",
    "\n",
    "    train_paths = [image_paths[i] for i in train_idx]\n",
    "    train_labels_list = [labels[i] for i in train_idx]\n",
    "    val_paths = [image_paths[i] for i in val_idx]\n",
    "    val_labels_list = [labels[i] for i in val_idx]\n",
    "\n",
    "    print(f\"  Train samples: {len(train_paths)}\")\n",
    "    print(f\"  Val samples: {len(val_paths)}\")\n",
    "\n",
    "    # ============= Create Datasets =============\n",
    "    train_tf, val_tf = get_augmentation_transforms()\n",
    "    \n",
    "    train_dataset = PreCachedImageDataset(\n",
    "        train_paths, train_labels_list, \n",
    "        transform=train_tf, \n",
    "        cache_images=CONFIG['cache_images'],\n",
    "        img_size=CONFIG['img_size'],\n",
    "        is_train=True\n",
    "    )\n",
    "    val_dataset = PreCachedImageDataset(\n",
    "        val_paths, val_labels_list,\n",
    "        transform=None,\n",
    "        cache_images=CONFIG['cache_images'],\n",
    "        img_size=CONFIG['img_size'],\n",
    "        is_train=False\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "    )\n",
    "\n",
    "    # ============= Build Model =============\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Building model...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    model = build_model(\n",
    "        model_name=CONFIG['model_name'],\n",
    "        num_classes=num_classes,\n",
    "        dropout_rate=CONFIG['dropout'],\n",
    "        freeze_backbone=CONFIG['freeze_backbone']\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"  Model: {CONFIG['model_name']}\")\n",
    "    print(f\"  Freeze backbone: {CONFIG['freeze_backbone']}\")\n",
    "    print(f\"  Dropout: {CONFIG['dropout']}\")\n",
    "    print(f\"  Trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "    # ============= Loss & Optimizer =============\n",
    "    if CONFIG['use_class_weights']:\n",
    "        train_counts = np.bincount(train_labels_list, minlength=num_classes)\n",
    "        class_weights = 1.0 / (train_counts + 1e-6)\n",
    "        class_weights = class_weights * (len(train_labels_list) / class_weights.sum())\n",
    "        class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "        print(f\"\\n  Using class weights: {class_weights.cpu().numpy()}\")\n",
    "    else:\n",
    "        class_weights = None\n",
    "    \n",
    "    if CONFIG['use_focal_loss']:\n",
    "        criterion = FocalLoss(alpha=class_weights, gamma=CONFIG['focal_gamma'])\n",
    "        print(f\"  Using Focal Loss (gamma={CONFIG['focal_gamma']})\")\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        print(f\"  Using Cross Entropy Loss\")\n",
    "\n",
    "    # ============= Training =============\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Starting training...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if CONFIG['use_gradual_unfreezing']:\n",
    "        best_val_acc, best_epoch = train_with_gradual_unfreezing(\n",
    "            model, train_loader, val_loader, criterion, device, CONFIG, class_names\n",
    "        )\n",
    "    else:\n",
    "        # Standard training loop would go here\n",
    "        pass\n",
    "\n",
    "    # ============= Training Complete =============\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Training Complete!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"  Best Val Acc: {best_val_acc:.2f}% (Epoch {best_epoch})\")\n",
    "    print(f\"  Model saved to: {CONFIG['save_dir']}/best_model.pth\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_grad (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
