{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab2516fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#importing standard python libraries for file system operations, randomness, path handling, data structure utilities and type hinting \n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from typing import Tuple, List\n",
    "\n",
    "#import numpy for numerical operations and PIL images for loading and manipulating images\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "#import core torch modules for building the neural network\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "#import the computer vision libraries for augmentations(transforms), models(pretrained backbone), etc\n",
    "from torchvision import transforms, models\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce126d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#                  1. Label Smoothing Loss\n",
    "# ============================================================\n",
    "\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    \"\"\"\n",
    "    Label smoothing prevents overconfident predictions and improves generalization.\n",
    "    This is crucial for reaching 75-80% accuracy.\n",
    "    \"\"\"\n",
    "    #initialize the loss functino with smoothing factor of 0.1 and optional weight for handling class imbalance\n",
    "    def __init__(self, epsilon: float = 0.1, weight=None):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.weight = weight\n",
    "    #define the forward pass of the loss     \n",
    "    def forward(self, outputs, targets):\n",
    "        n_classes = outputs.size(-1) #determines number of classes from size of model's raw output tensor\n",
    "        log_preds = F.log_softmax(outputs, dim=-1) #calculate log-softmax of model's outputs needed for loss calculation\n",
    "        \n",
    "        # Smooth labels\n",
    "        with torch.no_grad():   #disables gradient calculation as the target distribution is not learned (the labels do not require gradients because they are constants)\n",
    "            true_dist = torch.zeros_like(log_preds) #initializing tensor that will hold the smoothed target probabilities\n",
    "            true_dist.fill_(self.epsilon / (n_classes - 1)) #true_dist represents target distribution for 1 example, this fn assigns a small equal probability to every incorrect class when applying label smoothing\n",
    "            true_dist.scatter_(1, targets.unsqueeze(1), 1.0 - self.epsilon) #sets probability for correct class to be 1-epsilon\n",
    "            \n",
    "            # Apply per-class weights if provided to penalize misclassifications of rare emotions more heavily\n",
    "            if self.weight is not None:\n",
    "                true_dist = true_dist * self.weight.unsqueeze(0)\n",
    "        \n",
    "        loss = torch.sum(-true_dist * log_preds, dim=-1) #computes loss, kullback-leibler divergence is used here, which is equivalent to crossentropy\n",
    "        return loss.mean()  #return mean loss across entire batch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "398b2b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#                  2. Advanced Mixup with CutMix\n",
    "# ============================================================\n",
    "\n",
    "def mixup_data(x, y, alpha=0.4, device='cuda'): #define mixup function\n",
    "    \"\"\"Enhanced mixup for better generalization\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "        lam = max(lam, 1 - lam)  # Ensure lam >= 0.5 for stability\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "def cutmix_data(x, y, alpha=1.0, device='cuda'):    #define cutmix function\n",
    "    \"\"\"\n",
    "    CutMix augmentation: randomly cuts and pastes patches between images.\n",
    "    Better for facial features than standard mixup.\n",
    "    \"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "\n",
    "    # Get random box\n",
    "    W, H = x.size(2), x.size(3)\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "\n",
    "    # Uniform sampling\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    # Apply cutmix\n",
    "    x_cutmix = x.clone()\n",
    "    x_cutmix[:, :, bbx1:bbx2, bby1:bby2] = x[index, :, bbx1:bbx2, bby1:bby2]    #replaces the defined patch in the original image with the corresponding patch from a shuffled image\n",
    "    \n",
    "    # Adjust lambda to exactly match pixel ratio\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (W * H))\n",
    "    \n",
    "    y_a, y_b = y, y[index]\n",
    "    return x_cutmix, y_a, y_b, lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b9ac795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#                  3. Data Collection\n",
    "# ============================================================\n",
    "\n",
    "def gather_image_paths_and_labels(root_dir: str) -> Tuple[List[str], List[int], List[str]]:\n",
    "    \"\"\"Collect all images from emotion class subdirectories.\"\"\"\n",
    "    root = Path(root_dir)\n",
    "    if not root.exists():\n",
    "        raise FileNotFoundError(f\"Dataset root not found: {root_dir}\")\n",
    "\n",
    "    image_data = []\n",
    "    exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\"}\n",
    "    \n",
    "    for img_path in root.rglob(\"*\"): #recursively search the root directory\n",
    "        if img_path.suffix.lower() in exts: #check if file is within the extensions\n",
    "            emotion_class = img_path.parent.name    #infers emotion label from parent folder name \n",
    "            if emotion_class in ['train', 'test']:\n",
    "                emotion_class = img_path.parent.parent.name\n",
    "            \n",
    "            image_data.append((str(img_path), emotion_class))\n",
    "    \n",
    "    if not image_data:\n",
    "        raise ValueError(f\"No images found under {root_dir}\")\n",
    "    \n",
    "    #here we identify and sort the unique emotion names, then create a mapping from name to index\n",
    "    unique_classes = sorted(set(emotion for _, emotion in image_data))\n",
    "    class_to_idx = {name: idx for idx, name in enumerate(unique_classes)}\n",
    "    \n",
    "    image_paths = [path for path, _ in image_data]\n",
    "    labels = [class_to_idx[emotion] for _, emotion in image_data]\n",
    "    \n",
    "    return image_paths, labels, unique_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f37f1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#                  4. Enhanced Pre-cached Dataset\n",
    "# ============================================================\n",
    "\n",
    "class PreCachedImageDataset(Dataset):   #class designed to load all image data into RAM before training for max I/O speed\n",
    "    \"\"\"Ultra-fast dataset with pre-cached tensors.\"\"\"\n",
    "    def __init__(self, image_paths: List[str], labels: List[int], \n",
    "                 transform=None, cache_images: bool = True, img_size: int = 224,\n",
    "                 is_train: bool = True):\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "        self.cached_tensors = []\n",
    "        \n",
    "        if cache_images:    #check configuration before starting the caching process \n",
    "            print(f\"Pre-loading {len(image_paths)} images...\")\n",
    "            \n",
    "            #defining imagenet mean and std used for normalization \n",
    "            imagenet_mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "            imagenet_std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "            \n",
    "            #resize image and convert to pytorch tensor \n",
    "            resize_transform = transforms.Compose([\n",
    "                transforms.Resize(img_size),\n",
    "                transforms.ToTensor(),\n",
    "            ])\n",
    "            \n",
    "            for path in tqdm(image_paths, desc=\"Caching\"):\n",
    "                try:\n",
    "                    img = Image.open(path).convert(\"RGB\")\n",
    "                    img_tensor = resize_transform(img)\n",
    "                    \n",
    "                    if not is_train:\n",
    "                        img_tensor = (img_tensor - imagenet_mean) / imagenet_std\n",
    "                    \n",
    "                    self.cached_tensors.append(img_tensor)\n",
    "                except Exception as e:\n",
    "                    blank = torch.zeros(3, img_size, img_size)\n",
    "                    if not is_train:\n",
    "                        blank = (blank - imagenet_mean) / imagenet_std\n",
    "                    self.cached_tensors.append(blank)\n",
    "        else:\n",
    "            self.cached_tensors = None\n",
    "            self.image_paths = image_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.cached_tensors is not None:\n",
    "            img = self.cached_tensors[idx]\n",
    "            if self.is_train and self.transform:    #applying the augmentations on the fly if it is a train_sample \n",
    "                img = self.transform(img)\n",
    "        else:\n",
    "            img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "        \n",
    "        return img, self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90e8e60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#                  5. Advanced Transforms\n",
    "# ============================================================\n",
    "\n",
    "def get_augmentation_transforms() -> Tuple[transforms.Compose, transforms.Compose]:\n",
    "    \"\"\"\n",
    "    Strong augmentation strategy for better generalization.\n",
    "    \"\"\"\n",
    "    #defining the nornmalization parameters\n",
    "    imagenet_mean = [0.485, 0.456, 0.406]\n",
    "    imagenet_std = [0.229, 0.224, 0.225]\n",
    "    \n",
    "    # Training: Strong augmentation\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(20),  # Increased rotation\n",
    "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.1),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.15, 0.15), scale=(0.85, 1.15)),\n",
    "        transforms.RandomPerspective(distortion_scale=0.2, p=0.3),  # Add perspective\n",
    "        transforms.Normalize(imagenet_mean, imagenet_std),\n",
    "        transforms.RandomErasing(p=0.3, scale=(0.02, 0.2)),  # Stronger erasing\n",
    "    ])\n",
    "    \n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Normalize(imagenet_mean, imagenet_std),\n",
    "    ])\n",
    "    \n",
    "    return train_transform, val_transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "494674ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#                  6. Enhanced Model with Attention\n",
    "# ============================================================\n",
    "\n",
    "def build_model(model_name: str = 'resnet34', num_classes: int = 7, \n",
    "                dropout_rate: float = 0.5) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Build enhanced ResNet with better classifier.\n",
    "    Note: SE attention is added to layer4 of the backbone, not the fc layer.\n",
    "    \"\"\"\n",
    "    if model_name == 'resnet18':\n",
    "        weights = models.ResNet18_Weights.IMAGENET1K_V1\n",
    "        model = models.resnet18(weights=weights)\n",
    "    elif model_name == 'resnet34':\n",
    "        weights = models.ResNet34_Weights.IMAGENET1K_V1\n",
    "        model = models.resnet34(weights=weights)\n",
    "    elif model_name == 'resnet50':\n",
    "        weights = models.ResNet50_Weights.IMAGENET1K_V1\n",
    "        model = models.resnet50(weights=weights)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "\n",
    "    in_features = model.fc.in_features\n",
    "    \n",
    "    # Enhanced 3-layer classifier for better capacity\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.BatchNorm1d(in_features),\n",
    "        nn.Dropout(dropout_rate * 0.5),\n",
    "        nn.Linear(in_features, 1024),\n",
    "        nn.BatchNorm1d(1024),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(dropout_rate),\n",
    "        nn.Linear(1024, 512),\n",
    "        nn.BatchNorm1d(512),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(dropout_rate),\n",
    "        nn.Linear(512, num_classes)\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71d5f980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#                  7. Training Functions with Advanced Augmentation\n",
    "# ============================================================\n",
    "\n",
    "def train_one_epoch(model: nn.Module, loader: DataLoader, criterion: nn.Module,\n",
    "                   optimizer: optim.Optimizer, device: torch.device, epoch: int,\n",
    "                   scaler: GradScaler = None, use_mixup: bool = True, \n",
    "                   mixup_alpha: float = 0.4, use_amp: bool = False,\n",
    "                   use_cutmix: bool = True) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Training with mixup/cutmix alternation for better generalization.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch} [Train]\", leave=False)\n",
    "\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        # Randomly choose between mixup and cutmix\n",
    "        use_aug = random.random() < 0.5\n",
    "        use_cutmix_now = use_cutmix and random.random() < 0.5\n",
    "        \n",
    "        if use_amp and scaler is not None:\n",
    "            with autocast(device_type='cuda'):\n",
    "                if use_aug and use_mixup:\n",
    "                    if use_cutmix_now:\n",
    "                        mixed_images, labels_a, labels_b, lam = cutmix_data(images, labels, 1.0, device)\n",
    "                    else:\n",
    "                        mixed_images, labels_a, labels_b, lam = mixup_data(images, labels, mixup_alpha, device)\n",
    "                    \n",
    "                    outputs = model(mixed_images)\n",
    "                    loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)\n",
    "                    \n",
    "                    preds = outputs.argmax(dim=1)\n",
    "                    correct += (lam * (preds == labels_a).sum().item() + (1 - lam) * (preds == labels_b).sum().item())\n",
    "                else:\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    preds = outputs.argmax(dim=1)\n",
    "                    correct += (preds == labels).sum().item()\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            if use_aug and use_mixup:\n",
    "                if use_cutmix_now:\n",
    "                    mixed_images, labels_a, labels_b, lam = cutmix_data(images, labels, 1.0, device)\n",
    "                else:\n",
    "                    mixed_images, labels_a, labels_b, lam = mixup_data(images, labels, mixup_alpha, device)\n",
    "                \n",
    "                outputs = model(mixed_images)\n",
    "                loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)\n",
    "                \n",
    "                preds = outputs.argmax(dim=1)\n",
    "                correct += (lam * (preds == labels_a).sum().item() + (1 - lam) * (preds == labels_b).sum().item())\n",
    "            else:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                preds = outputs.argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        total += images.size(0)\n",
    "\n",
    "        if pbar.n % max(1, len(loader) // 20) == 0:\n",
    "            avg_loss = running_loss / (pbar.n + 1)\n",
    "            avg_acc = 100.0 * correct / total\n",
    "            pbar.set_postfix({\"loss\": f\"{avg_loss:.4f}\", \"acc\": f\"{avg_acc:.2f}%\"})\n",
    "\n",
    "    avg_loss = running_loss / len(loader)\n",
    "    avg_acc = 100.0 * correct / total\n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbf8f224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, loader: DataLoader, criterion: nn.Module,\n",
    "            device: torch.device, epoch: int, use_amp: bool = False,\n",
    "            use_tta: bool = False) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Evaluation with optional Test-Time Augmentation.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch} [Val]\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in pbar:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            if use_amp:\n",
    "                with autocast(device_type='cuda'):\n",
    "                    if use_tta:\n",
    "                        # Test-Time Augmentation: horizontal flip\n",
    "                        outputs1 = model(images)\n",
    "                        outputs2 = model(torch.flip(images, dims=[3]))\n",
    "                        outputs = (outputs1 + outputs2) / 2\n",
    "                    else:\n",
    "                        outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "            else:\n",
    "                if use_tta:\n",
    "                    outputs1 = model(images)\n",
    "                    outputs2 = model(torch.flip(images, dims=[3]))\n",
    "                    outputs = (outputs1 + outputs2) / 2\n",
    "                else:\n",
    "                    outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += images.size(0)\n",
    "\n",
    "            if pbar.n % max(1, len(loader) // 10) == 0:\n",
    "                avg_loss = running_loss / (pbar.n + 1)\n",
    "                avg_acc = 100.0 * correct / total\n",
    "                pbar.set_postfix({\"loss\": f\"{avg_loss:.4f}\", \"acc\": f\"{avg_acc:.2f}%\"})\n",
    "\n",
    "    avg_loss = running_loss / len(loader)\n",
    "    avg_acc = 100.0 * correct / total\n",
    "    return avg_loss, avg_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "109647a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#                  8. Main Training with OneCycleLR\n",
    "# ============================================================\n",
    "\n",
    "def train_advanced(model, train_loader, val_loader, criterion, \n",
    "                   device, CONFIG, class_names):\n",
    "    \"\"\"\n",
    "    Advanced training with OneCycleLR for better convergence.\n",
    "    \"\"\"\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=CONFIG['lr'],\n",
    "        weight_decay=CONFIG['weight_decay']\n",
    "    )\n",
    "    \n",
    "    # OneCycleLR: proven to reach higher accuracy faster\n",
    "    total_steps = len(train_loader) * CONFIG['epochs']\n",
    "    scheduler = OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=CONFIG['lr'] * 10,  # Peak LR\n",
    "        total_steps=total_steps,\n",
    "        pct_start=0.3,  # Warmup for 30% of training\n",
    "        anneal_strategy='cos',\n",
    "        div_factor=25.0,\n",
    "        final_div_factor=10000.0\n",
    "    )\n",
    "    \n",
    "    use_amp = torch.cuda.is_available()\n",
    "    scaler = GradScaler() if use_amp else None\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Starting high-accuracy training...\")\n",
    "    if use_amp:\n",
    "        print(\"Mixed Precision: ENABLED\")\n",
    "    else:\n",
    "        print(\"Mixed Precision: DISABLED (CPU mode)\")\n",
    "    print(f\"Target: 75-80% validation accuracy\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for epoch in range(1, CONFIG['epochs'] + 1):\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, device, epoch,\n",
    "            scaler, use_mixup=CONFIG['use_mixup'], \n",
    "            mixup_alpha=CONFIG['mixup_alpha'], use_amp=use_amp,\n",
    "            use_cutmix=CONFIG['use_cutmix']\n",
    "        )\n",
    "        \n",
    "        # Use TTA after epoch 20 for better validation accuracy\n",
    "        use_tta = CONFIG['use_tta'] and epoch > 20\n",
    "        val_loss, val_acc = evaluate(\n",
    "            model, val_loader, criterion, device, epoch, \n",
    "            use_amp=use_amp, use_tta=use_tta\n",
    "        )\n",
    "        \n",
    "        # Calculate overfitting gap\n",
    "        gap = train_acc - val_acc\n",
    "        \n",
    "        print(f\"\\n[Epoch {epoch}/{CONFIG['epochs']}]\")\n",
    "        print(f\"  Train -> Loss: {train_loss:.4f}  Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Val   -> Loss: {val_loss:.4f}  Acc: {val_acc:.2f}%\")\n",
    "        print(f\"  Gap: {gap:.2f}% | LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        if use_tta:\n",
    "            print(f\"  TTA: Enabled\")\n",
    "        \n",
    "        # Step scheduler per batch (OneCycleLR requirement handled in training loop)\n",
    "        # scheduler.step() is called per batch inside train_one_epoch\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = epoch\n",
    "            patience_counter = 0\n",
    "            save_checkpoint(model, optimizer, epoch, val_acc, val_loss, \n",
    "                          class_names, CONFIG)\n",
    "            print(f\"  ✓ New best! Val Acc: {val_acc:.2f}% (Gap: {gap:.2f}%)\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  No improvement ({patience_counter}/{CONFIG['patience']})\")\n",
    "            \n",
    "            if patience_counter >= CONFIG['patience']:\n",
    "                print(f\"\\n{'='*60}\")\n",
    "                print(f\"Early stopping triggered\")\n",
    "                print(\"=\"*60)\n",
    "                break\n",
    "    \n",
    "    return best_val_acc, best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47fe4ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, val_acc, val_loss, class_names, CONFIG):\n",
    "    \"\"\"Save model checkpoint\"\"\"\n",
    "    best_path = os.path.join(CONFIG['save_dir'], \"best_model.pth\")\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'val_acc': val_acc,\n",
    "        'val_loss': val_loss,\n",
    "        'class_names': class_names,\n",
    "        'config': CONFIG\n",
    "    }, best_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a19a2af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#                  9. Main Pipeline\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    # ============= High-Accuracy Configuration =============\n",
    "    CONFIG = {\n",
    "        'data_root': 'C:/adam/AMIT_Diploma/grad_project/archive_v1',\n",
    "        'model_name': 'resnet34',  # ResNet34 for better capacity\n",
    "        'img_size': 224,\n",
    "        'batch_size': 64,  # Smaller batch for better generalization\n",
    "        'epochs': 60,  # Extended training\n",
    "        'lr': 3e-4,  # Lower LR for stability\n",
    "        'weight_decay': 2e-4,  # Stronger regularization\n",
    "        'dropout': 0.5,\n",
    "        'val_size': 0.15,\n",
    "        'random_seed': 42,\n",
    "        'num_workers': 0,\n",
    "        'patience': 12,  # More patience for convergence\n",
    "        'use_class_weights': True,\n",
    "        'use_label_smoothing': True,  # Key for generalization\n",
    "        'label_smooth_eps': 0.1,\n",
    "        'use_mixup': True,\n",
    "        'mixup_alpha': 0.4,  # Stronger mixup\n",
    "        'use_cutmix': True,  # Add CutMix\n",
    "        'use_tta': True,  # Test-Time Augmentation\n",
    "        'save_dir': './checkpoints',\n",
    "        'cache_images': True,\n",
    "    }\n",
    "\n",
    "    # ============= Setup =============\n",
    "    random.seed(CONFIG['random_seed'])\n",
    "    np.random.seed(CONFIG['random_seed'])\n",
    "    torch.manual_seed(CONFIG['random_seed'])\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(CONFIG['random_seed'])\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    os.makedirs(CONFIG['save_dir'], exist_ok=True)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        print(f\"WARNING: CPU training - will take 6-10 hours\")\n",
    "\n",
    "    # ============= Load Data =============\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Loading dataset...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    image_paths, labels, class_names = gather_image_paths_and_labels(CONFIG['data_root'])\n",
    "    num_classes = len(class_names)\n",
    "    \n",
    "    print(f\"\\nDataset Statistics:\")\n",
    "    print(f\"  Total images: {len(image_paths)}\")\n",
    "    print(f\"  Classes ({num_classes}): {class_names}\")\n",
    "    \n",
    "    label_counts = Counter(labels)\n",
    "    print(f\"\\n  Class distribution:\")\n",
    "    for cls_idx, cls_name in enumerate(class_names):\n",
    "        print(f\"    {cls_name}: {label_counts[cls_idx]}\")\n",
    "\n",
    "    # ============= Stratified Split =============\n",
    "    print(f\"\\nCreating stratified split (val_size={CONFIG['val_size']})...\")\n",
    "    \n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=CONFIG['val_size'],\n",
    "                                random_state=CONFIG['random_seed'])\n",
    "    indices = np.arange(len(labels))\n",
    "    train_idx, val_idx = next(sss.split(indices, labels))\n",
    "\n",
    "    train_paths = [image_paths[i] for i in train_idx]\n",
    "    train_labels_list = [labels[i] for i in train_idx]\n",
    "    val_paths = [image_paths[i] for i in val_idx]\n",
    "    val_labels_list = [labels[i] for i in val_idx]\n",
    "\n",
    "    print(f\"  Train samples: {len(train_paths)}\")\n",
    "    print(f\"  Val samples: {len(val_paths)}\")\n",
    "\n",
    "    # ============= Create Datasets =============\n",
    "    train_tf, val_tf = get_augmentation_transforms()\n",
    "    \n",
    "    train_dataset = PreCachedImageDataset(\n",
    "        train_paths, train_labels_list, \n",
    "        transform=train_tf, \n",
    "        cache_images=CONFIG['cache_images'],\n",
    "        img_size=CONFIG['img_size'],\n",
    "        is_train=True\n",
    "    )\n",
    "    val_dataset = PreCachedImageDataset(\n",
    "        val_paths, val_labels_list,\n",
    "        transform=None,\n",
    "        cache_images=CONFIG['cache_images'],\n",
    "        img_size=CONFIG['img_size'],\n",
    "        is_train=False\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "    )\n",
    "\n",
    "    # ============= Build Enhanced Model =============\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Building enhanced model...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    model = build_model(\n",
    "        model_name=CONFIG['model_name'],\n",
    "        num_classes=num_classes,\n",
    "        dropout_rate=CONFIG['dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"  Model: {CONFIG['model_name']}\")\n",
    "    print(f\"  Architecture: {CONFIG['model_name'].upper()} + 3-layer classifier\")\n",
    "    print(f\"  Trainable params: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "    # ============= Loss with Label Smoothing =============\n",
    "    if CONFIG['use_class_weights']:\n",
    "        train_counts = np.bincount(train_labels_list, minlength=num_classes)\n",
    "        class_weights = 1.0 / (train_counts + 1e-6)\n",
    "        class_weights = class_weights * (len(train_labels_list) / class_weights.sum())\n",
    "        class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "        print(f\"\\n  Class weights: {class_weights.cpu().numpy()}\")\n",
    "    else:\n",
    "        class_weights = None\n",
    "    \n",
    "    if CONFIG['use_label_smoothing']:\n",
    "        criterion = LabelSmoothingCrossEntropy(\n",
    "            epsilon=CONFIG['label_smooth_eps'], \n",
    "            weight=class_weights\n",
    "        )\n",
    "        print(f\"  Loss: Label Smoothing CE (eps={CONFIG['label_smooth_eps']})\")\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        print(f\"  Loss: Standard Cross Entropy\")\n",
    "\n",
    "    # ============= Training =============\n",
    "    best_val_acc, best_epoch = train_advanced(\n",
    "        model, train_loader, val_loader, criterion, device, CONFIG, class_names\n",
    "    )\n",
    "\n",
    "    # ============= Final Results =============\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Training Complete!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"  Best Val Acc: {best_val_acc:.2f}% (Epoch {best_epoch})\")\n",
    "    print(f\"  Target Range: 75-80%\")\n",
    "    if best_val_acc >= 75:\n",
    "        print(f\"  ✓ TARGET ACHIEVED!\")\n",
    "    else:\n",
    "        print(f\"  Gap to target: {75 - best_val_acc:.2f}%\")\n",
    "    print(f\"  Model saved to: {CONFIG['save_dir']}/best_model.pth\")\n",
    "\n",
    "    # ============= Final Results =============\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Training Complete!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"  Best Val Acc: {best_val_acc:.2f}% (Epoch {best_epoch})\")\n",
    "    print(f\"  Target Range: 75-80%\")\n",
    "    if best_val_acc >= 75:\n",
    "        print(f\"  ✓ TARGET ACHIEVED!\")\n",
    "    else:\n",
    "        print(f\"  Gap to target: {75 - best_val_acc:.2f}%\")\n",
    "    print(f\"  Model saved to: {CONFIG['save_dir']}/best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8d893f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "\n",
      "============================================================\n",
      "Loading dataset...\n",
      "============================================================\n",
      "\n",
      "Dataset Statistics:\n",
      "  Total images: 35887\n",
      "  Classes (7): ['angry', 'disgusted', 'fearful', 'happy', 'neutral', 'sad', 'surprised']\n",
      "\n",
      "  Class distribution:\n",
      "    angry: 4953\n",
      "    disgusted: 547\n",
      "    fearful: 5121\n",
      "    happy: 8989\n",
      "    neutral: 6198\n",
      "    sad: 6077\n",
      "    surprised: 4002\n",
      "\n",
      "Creating stratified split (val_size=0.15)...\n",
      "  Train samples: 30503\n",
      "  Val samples: 5384\n",
      "Pre-loading 30503 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching:   8%|▊         | 2321/30503 [00:26<03:08, 149.13it/s]"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90265467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea15054d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov 30 17:03:01 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 581.29                 Driver Version: 581.29         CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4060 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   36C    P0              9W /   80W |       0MiB /   8188MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a042307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bebb23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_gradproj (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
