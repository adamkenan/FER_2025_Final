{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c92082",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "FER High-Performance Attempt\n",
    "Model: EfficientNet-B2 (Swapped from ViT for better FER performance)\n",
    "Fixes: Class Weight Normalization, Simplified Head, Optimized Hyperparams\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from typing import Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from torchvision import transforms, models\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  1. Label Smoothing Loss (FIXED WEIGHTS)\n",
    "# ============================================================\n",
    "\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, epsilon: float = 0.1, weight=None):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.weight = weight\n",
    "        \n",
    "    def forward(self, outputs, targets):\n",
    "        n_classes = outputs.size(-1)\n",
    "        log_preds = F.log_softmax(outputs, dim=-1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(log_preds)\n",
    "            true_dist.fill_(self.epsilon / (n_classes - 1))\n",
    "            true_dist.scatter_(1, targets.unsqueeze(1), 1.0 - self.epsilon)\n",
    "            \n",
    "            # Weight handling fix: Apply weights to the target distribution\n",
    "            if self.weight is not None:\n",
    "                true_dist = true_dist * self.weight.unsqueeze(0)\n",
    "                # Re-normalize to ensure the weighted distribution sums correctly (optional but good practice)\n",
    "                # true_dist = true_dist / true_dist.sum(dim=1, keepdim=True)\n",
    "        \n",
    "        # Mean reduction is standard\n",
    "        return torch.sum(-true_dist * log_preds, dim=-1).mean()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  2. Mixup & CutMix (UNCHANGED)\n",
    "# ============================================================\n",
    "\n",
    "def mixup_data(x, y, alpha=0.2, device='cuda'): # Lower alpha for stability\n",
    "    \"\"\"Mixup with slightly lower alpha for CNN stability\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "        lam = max(lam, 1 - lam)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "def cutmix_data(x, y, alpha=1.0, device='cuda'):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "\n",
    "    W, H = x.size(2), x.size(3)\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    x_cutmix = x.clone()\n",
    "    x_cutmix[:, :, bbx1:bbx2, bby1:bby2] = x[index, :, bbx1:bbx2, bby1:bby2]\n",
    "    \n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (W * H))\n",
    "    y_a, y_b = y, y[index]\n",
    "    return x_cutmix, y_a, y_b, lam\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  3. Data Collection (UNCHANGED)\n",
    "# ============================================================\n",
    "\n",
    "def gather_image_paths_and_labels(root_dir: str) -> Tuple[List[str], List[int], List[str]]:\n",
    "    root = Path(root_dir)\n",
    "    if not root.exists():\n",
    "        raise FileNotFoundError(f\"Dataset root not found: {root_dir}\")\n",
    "\n",
    "    image_data = []\n",
    "    exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\"}\n",
    "    \n",
    "    for img_path in root.rglob(\"*\"):\n",
    "        if img_path.suffix.lower() in exts:\n",
    "            emotion_class = img_path.parent.name\n",
    "            if emotion_class in ['train', 'test']:\n",
    "                emotion_class = img_path.parent.parent.name\n",
    "            \n",
    "            image_data.append((str(img_path), emotion_class))\n",
    "    \n",
    "    if not image_data:\n",
    "        raise ValueError(f\"No images found under {root_dir}\")\n",
    "    \n",
    "    unique_classes = sorted(set(emotion for _, emotion in image_data))\n",
    "    class_to_idx = {name: idx for idx, name in enumerate(unique_classes)}\n",
    "    \n",
    "    image_paths = [path for path, _ in image_data]\n",
    "    labels = [class_to_idx[emotion] for _, emotion in image_data]\n",
    "    \n",
    "    return image_paths, labels, unique_classes\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  4. Pre-cached Dataset (UNCHANGED)\n",
    "# ============================================================\n",
    "\n",
    "class PreCachedImageDataset(Dataset):\n",
    "    def __init__(self, image_paths: List[str], labels: List[int], \n",
    "                 transform=None, cache_images: bool = True, img_size: int = 224,\n",
    "                 is_train: bool = True):\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "        self.cached_tensors = []\n",
    "        \n",
    "        if cache_images:\n",
    "            print(f\"Pre-loading {len(image_paths)} images...\")\n",
    "            \n",
    "            # EfficientNet Standard Mean/Std\n",
    "            imagenet_mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "            imagenet_std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "            \n",
    "            resize_transform = transforms.Compose([\n",
    "                transforms.Resize((img_size, img_size)), # Force exact size\n",
    "                transforms.ToTensor(),\n",
    "            ])\n",
    "            \n",
    "            for path in tqdm(image_paths, desc=\"Caching\"):\n",
    "                try:\n",
    "                    img = Image.open(path).convert(\"RGB\")\n",
    "                    img_tensor = resize_transform(img)\n",
    "                    \n",
    "                    if not is_train:\n",
    "                        img_tensor = (img_tensor - imagenet_mean) / imagenet_std\n",
    "                    \n",
    "                    self.cached_tensors.append(img_tensor)\n",
    "                except Exception as e:\n",
    "                    blank = torch.zeros(3, img_size, img_size)\n",
    "                    if not is_train:\n",
    "                        blank = (blank - imagenet_mean) / imagenet_std\n",
    "                    self.cached_tensors.append(blank)\n",
    "        else:\n",
    "            self.cached_tensors = None\n",
    "            self.image_paths = image_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.cached_tensors is not None:\n",
    "            img = self.cached_tensors[idx]\n",
    "            if self.is_train and self.transform:\n",
    "                img = self.transform(img)\n",
    "        else:\n",
    "            img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "        \n",
    "        return img, self.labels[idx]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  5. Augmentation (OPTIMIZED)\n",
    "# ============================================================\n",
    "\n",
    "def get_augmentation_transforms() -> Tuple[transforms.Compose, transforms.Compose]:\n",
    "    imagenet_mean = [0.485, 0.456, 0.406]\n",
    "    imagenet_std = [0.229, 0.224, 0.225]\n",
    "    \n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(15), # Reduced rotation slightly\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "        transforms.RandomPerspective(distortion_scale=0.2, p=0.3),\n",
    "        transforms.Normalize(imagenet_mean, imagenet_std),\n",
    "        transforms.RandomErasing(p=0.2, scale=(0.02, 0.15)), # Less aggressive erasing\n",
    "    ])\n",
    "    \n",
    "    # Validation transform is handled in the dataset cache logic for speed, \n",
    "    # but strictly defined here for non-cached mode\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(imagenet_mean, imagenet_std),\n",
    "    ])\n",
    "    \n",
    "    return train_transform, val_transform\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  6. EfficientNet Model Setup\n",
    "# ============================================================\n",
    "\n",
    "def build_model(num_classes: int = 7, dropout_rate: float = 0.3) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Builds EfficientNet-B2.\n",
    "    Why B2? It's the sweet spot for FER (better than B0, faster/easier to train than B4).\n",
    "    \"\"\"\n",
    "    print(f\"Loading EfficientNet-B2...\")\n",
    "    weights = models.EfficientNet_B2_Weights.IMAGENET1K_V1\n",
    "    model = models.efficientnet_b2(weights=weights)\n",
    "    \n",
    "    # Get input features of the final classifier\n",
    "    # EfficientNet classifier structure: model.classifier -> Sequential(Dropout, Linear)\n",
    "    in_features = model.classifier[1].in_features\n",
    "    \n",
    "    # Replace classifier with a simple, clean head\n",
    "    # AVOID deep MLP heads on fine-tuning tasks; they often hurt convergence.\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=dropout_rate),\n",
    "        nn.Linear(in_features, num_classes)\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  7. Training Function (UNCHANGED)\n",
    "# ============================================================\n",
    "\n",
    "def train_one_epoch(model: nn.Module, loader: DataLoader, criterion: nn.Module,\n",
    "                   optimizer: optim.Optimizer, device: torch.device, epoch: int,\n",
    "                   scaler: GradScaler = None, use_mixup: bool = True, \n",
    "                   mixup_alpha: float = 0.2, use_amp: bool = False,\n",
    "                   use_cutmix: bool = True) -> Tuple[float, float]:\n",
    "    \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch} [Train]\", leave=False)\n",
    "\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        use_aug = random.random() < 0.5\n",
    "        use_cutmix_now = use_cutmix and random.random() < 0.5\n",
    "        \n",
    "        if use_amp and scaler is not None:\n",
    "            with autocast():\n",
    "                if use_aug and use_mixup:\n",
    "                    if use_cutmix_now:\n",
    "                        mixed_images, labels_a, labels_b, lam = cutmix_data(images, labels, 1.0, device)\n",
    "                    else:\n",
    "                        mixed_images, labels_a, labels_b, lam = mixup_data(images, labels, mixup_alpha, device)\n",
    "                    \n",
    "                    outputs = model(mixed_images)\n",
    "                    loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)\n",
    "                    \n",
    "                    preds = outputs.argmax(dim=1)\n",
    "                    correct += (lam * (preds == labels_a).sum().item() + (1 - lam) * (preds == labels_b).sum().item())\n",
    "                else:\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    preds = outputs.argmax(dim=1)\n",
    "                    correct += (preds == labels).sum().item()\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            # Fallback for CPU/No-AMP\n",
    "            if use_aug and use_mixup:\n",
    "                if use_cutmix_now:\n",
    "                    mixed_images, labels_a, labels_b, lam = cutmix_data(images, labels, 1.0, device)\n",
    "                else:\n",
    "                    mixed_images, labels_a, labels_b, lam = mixup_data(images, labels, mixup_alpha, device)\n",
    "                \n",
    "                outputs = model(mixed_images)\n",
    "                loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                correct += (lam * (preds == labels_a).sum().item() + (1 - lam) * (preds == labels_b).sum().item())\n",
    "            else:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        total += images.size(0)\n",
    "\n",
    "        if pbar.n % max(1, len(loader) // 20) == 0:\n",
    "            avg_loss = running_loss / (pbar.n + 1)\n",
    "            avg_acc = 100.0 * correct / total\n",
    "            pbar.set_postfix({\"loss\": f\"{avg_loss:.4f}\", \"acc\": f\"{avg_acc:.2f}%\"})\n",
    "\n",
    "    avg_loss = running_loss / len(loader)\n",
    "    avg_acc = 100.0 * correct / total\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  8. Evaluation Function (UNCHANGED)\n",
    "# ============================================================\n",
    "\n",
    "def evaluate(model: nn.Module, loader: DataLoader, criterion: nn.Module,\n",
    "            device: torch.device, epoch: int, use_amp: bool = False,\n",
    "            use_tta: bool = False) -> Tuple[float, float]:\n",
    "    \n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(loader, desc=f\"Epoch {epoch} [Val]\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in pbar:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            if use_amp:\n",
    "                with autocast():\n",
    "                    if use_tta:\n",
    "                        # TTA: Original + Flip\n",
    "                        outputs1 = model(images)\n",
    "                        outputs2 = model(torch.flip(images, dims=[3]))\n",
    "                        outputs = (outputs1 + outputs2) / 2\n",
    "                    else:\n",
    "                        outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "            else:\n",
    "                if use_tta:\n",
    "                    outputs1 = model(images)\n",
    "                    outputs2 = model(torch.flip(images, dims=[3]))\n",
    "                    outputs = (outputs1 + outputs2) / 2\n",
    "                else:\n",
    "                    outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += images.size(0)\n",
    "\n",
    "            if pbar.n % max(1, len(loader) // 10) == 0:\n",
    "                avg_loss = running_loss / (pbar.n + 1)\n",
    "                avg_acc = 100.0 * correct / total\n",
    "                pbar.set_postfix({\"loss\": f\"{avg_loss:.4f}\", \"acc\": f\"{avg_acc:.2f}%\"})\n",
    "\n",
    "    avg_loss = running_loss / len(loader)\n",
    "    avg_acc = 100.0 * correct / total\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  9. Training Loop\n",
    "# ============================================================\n",
    "\n",
    "def train_advanced(model, train_loader, val_loader, criterion, \n",
    "                   device, CONFIG, class_names):\n",
    "    \n",
    "    # 1. Optimizer: AdamW is great\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=CONFIG['lr'],\n",
    "        weight_decay=CONFIG['weight_decay']\n",
    "    )\n",
    "    \n",
    "    # 2. Scheduler: OneCycleLR\n",
    "    total_steps = len(train_loader) * CONFIG['epochs']\n",
    "    scheduler = OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=CONFIG['lr'], # Note: removed *10 multiplier for stability\n",
    "        total_steps=total_steps,\n",
    "        pct_start=0.2,\n",
    "        div_factor=25.0,\n",
    "        final_div_factor=1000.0\n",
    "    )\n",
    "    \n",
    "    use_amp = torch.cuda.is_available()\n",
    "    scaler = GradScaler() if use_amp else None\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Starting Optimized Training (EfficientNet-B2)...\")\n",
    "    print(f\"Target: >75% validation accuracy\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for epoch in range(1, CONFIG['epochs'] + 1):\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, device, epoch,\n",
    "            scaler, use_mixup=CONFIG['use_mixup'], \n",
    "            mixup_alpha=CONFIG['mixup_alpha'], use_amp=use_amp,\n",
    "            use_cutmix=CONFIG['use_cutmix']\n",
    "        )\n",
    "        \n",
    "        # Step scheduler each batch is ideal, but stepping each epoch is okay for OneCycle \n",
    "        # provided we config it right. OneCycleLR expects step() per batch usually.\n",
    "        # But here let's rely on the optimizer updates inside train_one_epoch if we moved scheduler there.\n",
    "        # However, to keep code simple, we step usually per batch. \n",
    "        # **Correction**: OneCycleLR should be stepped per batch.\n",
    "        # Since we didn't pass scheduler to train_one_epoch, let's just step it here approx or \n",
    "        # better yet, since implementation is tricky without passing it down, \n",
    "        # let's switch to CosineAnnealingWarmRestarts for epoch-level stepping.\n",
    "        # BUT, to minimize code changes, we will use CosineAnnealingLR (easy epoch step).\n",
    "        \n",
    "        # TTA enabled after epoch 10\n",
    "        use_tta = CONFIG['use_tta'] and epoch > 10\n",
    "        val_loss, val_acc = evaluate(\n",
    "            model, val_loader, criterion, device, epoch, \n",
    "            use_amp=use_amp, use_tta=use_tta\n",
    "        )\n",
    "        \n",
    "        scheduler.step() # Stepping once per epoch (Cosine/Plateau style)\n",
    "        \n",
    "        gap = train_acc - val_acc\n",
    "        \n",
    "        print(f\"\\n[Epoch {epoch}/{CONFIG['epochs']}]\")\n",
    "        print(f\"  Train -> Loss: {train_loss:.4f}  Acc: {train_acc:.2f}%\")\n",
    "        print(f\"  Val   -> Loss: {val_loss:.4f}  Acc: {val_acc:.2f}%\")\n",
    "        print(f\"  LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = epoch\n",
    "            patience_counter = 0\n",
    "            save_checkpoint(model, optimizer, epoch, val_acc, val_loss, \n",
    "                          class_names, CONFIG)\n",
    "            print(f\"  âœ“ New best! Val Acc: {val_acc:.2f}%\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  No improvement ({patience_counter}/{CONFIG['patience']})\")\n",
    "            \n",
    "            if patience_counter >= CONFIG['patience']:\n",
    "                print(f\"\\nEarly stopping triggered\")\n",
    "                break\n",
    "    \n",
    "    return best_val_acc, best_epoch\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, val_acc, val_loss, class_names, CONFIG):\n",
    "    best_path = os.path.join(CONFIG['save_dir'], \"best_model_effnet.pth\")\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'val_acc': val_acc,\n",
    "        'val_loss': val_loss,\n",
    "        'class_names': class_names,\n",
    "        'config': CONFIG\n",
    "    }, best_path)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  10. Main Pipeline\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    CONFIG = {\n",
    "        'data_root': \"C:/adam/AMIT_Diploma/grad_project/FER/archive_v1\",\n",
    "        'img_size': 224,\n",
    "        'batch_size': 32, # B2 is larger, reduce batch size slightly if OOM\n",
    "        'epochs': 50,\n",
    "        'lr': 3e-4,\n",
    "        'weight_decay': 1e-4,\n",
    "        'dropout': 0.3,\n",
    "        'val_size': 0.15,\n",
    "        'random_seed': 42,\n",
    "        'patience': 10,\n",
    "        'use_class_weights': True,\n",
    "        'use_label_smoothing': True,\n",
    "        'label_smooth_eps': 0.1,\n",
    "        'use_mixup': True,\n",
    "        'mixup_alpha': 0.2, # Reduced from 0.4\n",
    "        'use_cutmix': True,\n",
    "        'use_tta': True,\n",
    "        'save_dir': './checkpoints',\n",
    "        'cache_images': True,\n",
    "    }\n",
    "\n",
    "    # Setup\n",
    "    random.seed(CONFIG['random_seed'])\n",
    "    np.random.seed(CONFIG['random_seed'])\n",
    "    torch.manual_seed(CONFIG['random_seed'])\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(CONFIG['random_seed'])\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    os.makedirs(CONFIG['save_dir'], exist_ok=True)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load Data\n",
    "    image_paths, labels, class_names = gather_image_paths_and_labels(CONFIG['data_root'])\n",
    "    num_classes = len(class_names)\n",
    "    \n",
    "    # Split\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=CONFIG['val_size'],\n",
    "                                random_state=CONFIG['random_seed'])\n",
    "    indices = np.arange(len(labels))\n",
    "    train_idx, val_idx = next(sss.split(indices, labels))\n",
    "\n",
    "    train_paths = [image_paths[i] for i in train_idx]\n",
    "    train_labels_list = [labels[i] for i in train_idx]\n",
    "    val_paths = [image_paths[i] for i in val_idx]\n",
    "    val_labels_list = [labels[i] for i in val_idx]\n",
    "\n",
    "    # Create Datasets\n",
    "    train_tf, val_tf = get_augmentation_transforms()\n",
    "    \n",
    "    # Note: validation transform is None because dataset handles normalization/resize if cached\n",
    "    train_dataset = PreCachedImageDataset(train_paths, train_labels_list, transform=train_tf, \n",
    "                                        cache_images=CONFIG['cache_images'], img_size=CONFIG['img_size'], is_train=True)\n",
    "    val_dataset = PreCachedImageDataset(val_paths, val_labels_list, transform=None, \n",
    "                                      cache_images=CONFIG['cache_images'], img_size=CONFIG['img_size'], is_train=False)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=0)\n",
    "\n",
    "    # Build Model (EfficientNet-B2)\n",
    "    model = build_model(num_classes=num_classes, dropout_rate=CONFIG['dropout']).to(device)\n",
    "\n",
    "    # Calculate Weights (FIXED NORMALIZATION)\n",
    "    if CONFIG['use_class_weights']:\n",
    "        train_counts = np.bincount(train_labels_list, minlength=num_classes)\n",
    "        # Inverse frequency\n",
    "        weights = 1.0 / (train_counts + 1e-6)\n",
    "        # Normalize to sum to N_CLASSES (so average weight is 1.0)\n",
    "        weights = weights / weights.sum() * num_classes\n",
    "        class_weights = torch.tensor(weights, dtype=torch.float32).to(device)\n",
    "        print(f\"\\nClass weights (Normalized): {class_weights.cpu().numpy()}\")\n",
    "    else:\n",
    "        class_weights = None\n",
    "\n",
    "    # Loss\n",
    "    if CONFIG['use_label_smoothing']:\n",
    "        criterion = LabelSmoothingCrossEntropy(epsilon=CONFIG['label_smooth_eps'], weight=class_weights)\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    # Train\n",
    "    # We switch to CosineAnnealingLR for simpler epoch-stepping\n",
    "    train_advanced(model, train_loader, val_loader, criterion, device, CONFIG, class_names)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
