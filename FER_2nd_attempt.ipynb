{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f20d8898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing RetinaFace extractor...\n",
      "download_path: C:/adam/AMIT_Diploma/grad_project/insightface_models\\models\\buffalo_l\n",
      "Downloading C:/adam/AMIT_Diploma/grad_project/insightface_models\\models\\buffalo_l.zip from https://github.com/deepinsight/insightface/releases/download/v0.7/buffalo_l.zip...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 281857/281857 [00:52<00:00, 5347.75KB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:/adam/AMIT_Diploma/grad_project/insightface_models\\models\\buffalo_l\\1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:/adam/AMIT_Diploma/grad_project/insightface_models\\models\\buffalo_l\\2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:/adam/AMIT_Diploma/grad_project/insightface_models\\models\\buffalo_l\\det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:/adam/AMIT_Diploma/grad_project/insightface_models\\models\\buffalo_l\\genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: C:/adam/AMIT_Diploma/grad_project/insightface_models\\models\\buffalo_l\\w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n",
      "\n",
      "Extracting training features...\n",
      "Pre-extracting features from all images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22967/22967 [29:42<00:00, 12.89it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting validation features...\n",
      "Pre-extracting features from all images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5742/5742 [07:24<00:00, 12.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using device: cpu\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 1.8195  |  Acc 0.2527\n",
      "Val   Loss 1.8119  |  Acc 0.2534\n",
      "\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 1.8041  |  Acc 0.2587\n",
      "Val   Loss 1.8142  |  Acc 0.2532\n",
      "\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 1.7927  |  Acc 0.2625\n",
      "Val   Loss 1.8355  |  Acc 0.2546\n",
      "\n",
      "Epoch 4/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 1.7933  |  Acc 0.2632\n",
      "Val   Loss 1.8459  |  Acc 0.2539\n",
      "\n",
      "Epoch 5/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 1.7892  |  Acc 0.2638\n",
      "Val   Loss 1.8564  |  Acc 0.2536\n",
      "\n",
      "Epoch 6/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 1.7902  |  Acc 0.2642\n",
      "Val   Loss 1.8680  |  Acc 0.2539\n",
      "\n",
      "Epoch 7/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 1.7885  |  Acc 0.2646\n",
      "Val   Loss 1.8887  |  Acc 0.2544\n",
      "\n",
      "Epoch 8/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 1.7851  |  Acc 0.2650\n",
      "Val   Loss 1.8864  |  Acc 0.2539\n",
      "\n",
      "Epoch 9/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 1.7826  |  Acc 0.2657\n",
      "Val   Loss 1.8966  |  Acc 0.2541\n",
      "\n",
      "Epoch 10/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 1.7839  |  Acc 0.2657\n",
      "Val   Loss 1.9101  |  Acc 0.2536\n",
      "\n",
      "Epoch 11/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 1.7877  |  Acc 0.2653\n",
      "Val   Loss 1.9036  |  Acc 0.2541\n",
      "\n",
      "Epoch 12/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 1.7862  |  Acc 0.2652\n",
      "Val   Loss 1.9270  |  Acc 0.2534\n",
      "\n",
      "Epoch 13/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 1.7866  |  Acc 0.2654\n",
      "Val   Loss 1.9319  |  Acc 0.2541\n",
      "\n",
      "Epoch 14/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 1.7911  |  Acc 0.2651\n",
      "Val   Loss 1.9397  |  Acc 0.2532\n",
      "\n",
      "Epoch 15/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 1.7826  |  Acc 0.2658\n",
      "Val   Loss 1.9482  |  Acc 0.2539\n",
      "\n",
      "Model saved as 'fer_model_final.pth'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import insightface\n",
    "from insightface.app import FaceAnalysis\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from retinaface import RetinaFace\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from glob import glob\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from tqdm import tqdm\n",
    "from torchvision.models import efficientnet_b0\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                1. RetinaFace Feature Extractor\n",
    "# ============================================================\n",
    "\n",
    "class RetinaFaceFeatureExtractor:\n",
    "    def __init__(self):\n",
    "        from insightface.app import FaceAnalysis\n",
    "\n",
    "        self.detector = FaceAnalysis(\n",
    "            name=\"buffalo_l\",\n",
    "            providers=['CPUExecutionProvider'],\n",
    "            root=\"C:/adam/AMIT_Diploma/grad_project/insightface_models\"\n",
    "        )\n",
    "        self.detector.prepare(ctx_id=0, det_size=(640, 640))\n",
    "\n",
    "    def detect_and_extract(self, image):\n",
    "        \"\"\"Returns (feature_vector, cropped_face_image_or_None)\"\"\"\n",
    "\n",
    "        # Use InsightFace's get method instead of predict\n",
    "        faces = self.detector.get(image)\n",
    "\n",
    "        if len(faces) == 0:\n",
    "            return None, None\n",
    "\n",
    "        face = faces[0]\n",
    "        \n",
    "        # Get bounding box and ensure it's within image bounds\n",
    "        bbox = face.bbox.astype(int)\n",
    "        h, w = image.shape[:2]\n",
    "        \n",
    "        x1 = max(0, bbox[0])\n",
    "        y1 = max(0, bbox[1])\n",
    "        x2 = min(w, bbox[2])\n",
    "        y2 = min(h, bbox[3])\n",
    "        \n",
    "        # Check if bbox is valid\n",
    "        if x2 <= x1 or y2 <= y1:\n",
    "            return None, None\n",
    "        \n",
    "        # Crop face\n",
    "        face_img = image[y1:y2, x1:x2]\n",
    "        \n",
    "        # Double check the cropped image is not empty\n",
    "        if face_img.size == 0:\n",
    "            return None, None\n",
    "\n",
    "        # Resize input face to detector backbone input size (224 is ok)\n",
    "        face_resized = cv2.resize(face_img, (224, 224))\n",
    "        face_rgb = cv2.cvtColor(face_resized, cv2.COLOR_BGR2RGB)\n",
    "        face_tensor = torch.tensor(face_rgb).permute(2, 0, 1).float() / 255.\n",
    "        face_tensor = face_tensor.unsqueeze(0)\n",
    "\n",
    "        # Extract features using the face embedding (512-dim)\n",
    "        # Instead of trying to access backbone, use the embedding\n",
    "        features = torch.tensor(face.embedding)\n",
    "\n",
    "        return features, face_img  # feature_vector, cropped face\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                2. Pre-extracted Feature Dataset \n",
    "# ============================================================\n",
    "\n",
    "class PreExtractedFeatureDataset(Dataset):\n",
    "    \"\"\"Dataset that works with pre-extracted features\"\"\"\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#            3. EfficientNet Classifier (Feature Input)\n",
    "# ============================================================\n",
    "\n",
    "class EfficientNetFeatureClassifier(nn.Module):\n",
    "    def __init__(self, feature_dim=512, num_classes=7):\n",
    "        super().__init__()\n",
    "\n",
    "        # Load efficientnet_b0\n",
    "        base = efficientnet_b0(weights='IMAGENET1K_V1')\n",
    "\n",
    "        # Replace the entire feature extractor\n",
    "        # LN → ReLU → Dropout → classification\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                      4. Train / Val Functions\n",
    "# ============================================================\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss, correct = 0, 0\n",
    "\n",
    "    for features, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    return total_loss / len(loader), correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels in tqdm(loader, desc=\"Validating\", leave=False):\n",
    "            features, labels = features.to(device), labels.to(device)\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "    return total_loss / len(loader), correct / len(loader.dataset)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                  5. Feature Pre-extraction\n",
    "# ============================================================\n",
    "\n",
    "def extract_all_features(image_paths, labels, extractor):\n",
    "    \"\"\"Pre-extract features from all images once\"\"\"\n",
    "    features_list = []\n",
    "    valid_labels = []\n",
    "    skipped = 0\n",
    "    \n",
    "    print(\"Pre-extracting features from all images...\")\n",
    "    for img_path, label in tqdm(zip(image_paths, labels), total=len(image_paths)):\n",
    "        image = cv2.imread(img_path)\n",
    "        \n",
    "        # Check if image was read successfully\n",
    "        if image is None:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        features, face = extractor.detect_and_extract(image)\n",
    "        \n",
    "        if features is None:\n",
    "            # If detection fails, use zero-vector\n",
    "            features = torch.zeros(512)\n",
    "        \n",
    "        features_list.append(features.float())\n",
    "        valid_labels.append(label)\n",
    "    \n",
    "    if skipped > 0:\n",
    "        print(f\"Skipped {skipped} corrupted/unreadable images\")\n",
    "    \n",
    "    # Stack all features into a single tensor\n",
    "    features_tensor = torch.stack(features_list)\n",
    "    labels_tensor = torch.tensor(valid_labels, dtype=torch.long)\n",
    "    \n",
    "    return features_tensor, labels_tensor\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                        6. Main Script\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    data_path = \"C:/adam/AMIT_Diploma/grad_project/archive (1)/train\"\n",
    "\n",
    "    image_paths = glob(os.path.join(data_path, \"*/*.png\"))\n",
    "    labels = [os.path.basename(os.path.dirname(p)) for p in image_paths]\n",
    "\n",
    "    # Convert class names → integer IDs\n",
    "    class_to_idx = {c: i for i, c in enumerate(sorted(set(labels)))}\n",
    "    labels = [class_to_idx[c] for c in labels]\n",
    "\n",
    "    # Stratified split\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "    train_idx, val_idx = next(sss.split(image_paths, labels))\n",
    "\n",
    "    train_paths = [image_paths[i] for i in train_idx]\n",
    "    val_paths = [image_paths[i] for i in val_idx]\n",
    "\n",
    "    train_labels = [labels[i] for i in train_idx]\n",
    "    val_labels = [labels[i] for i in val_idx]\n",
    "\n",
    "    # Initialize extractor once\n",
    "    print(\"Initializing RetinaFace extractor...\")\n",
    "    extractor = RetinaFaceFeatureExtractor()\n",
    "\n",
    "    # Pre-extract all features\n",
    "    print(\"\\nExtracting training features...\")\n",
    "    train_features, train_labels_tensor = extract_all_features(train_paths, train_labels, extractor)\n",
    "    \n",
    "    print(\"\\nExtracting validation features...\")\n",
    "    val_features, val_labels_tensor = extract_all_features(val_paths, val_labels, extractor)\n",
    "\n",
    "    # Create datasets with pre-extracted features\n",
    "    train_dataset = PreExtractedFeatureDataset(train_features, train_labels_tensor)\n",
    "    val_dataset = PreExtractedFeatureDataset(val_features, val_labels_tensor)\n",
    "\n",
    "    # DataLoaders (can use more workers now since no heavy processing in __getitem__)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "    # Model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"\\nUsing device: {device}\")\n",
    "    \n",
    "    model = EfficientNetFeatureClassifier(feature_dim=512).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # Training\n",
    "    print(\"\\nStarting training...\")\n",
    "    for epoch in range(15):\n",
    "        print(f\"\\nEpoch {epoch+1}/15\")\n",
    "\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "\n",
    "        print(f\"Train Loss {train_loss:.4f}  |  Acc {train_acc:.4f}\")\n",
    "        print(f\"Val   Loss {val_loss:.4f}  |  Acc {val_acc:.4f}\")\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), \"fer_model_final.pth\")\n",
    "    print(\"\\nModel saved as 'fer_model_final.pth'\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_grad (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
